<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[LeetCode_Python3_1]]></title>
      <url>/2018/03/19/LeetCode-Python3-1/</url>
      <content type="html"></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[java面试题-1]]></title>
      <url>/2018/03/18/java%E9%9D%A2%E8%AF%95%E9%A2%98-1/</url>
      <content type="html"><![CDATA[<p>Java面试题1-20</p>
<h2 id="001-如果main方法被声明为private会怎样？"><a href="#001-如果main方法被声明为private会怎样？" class="headerlink" title="001 如果main方法被声明为private会怎样？"></a>001 如果main方法被声明为private会怎样？</h2><p>答：编译正常，运行提示<code>main方法不是public的</code>。</p>
<h2 id="002-Java传引用和传值的区别。"><a href="#002-Java传引用和传值的区别。" class="headerlink" title="002 Java传引用和传值的区别。"></a>002 Java传引用和传值的区别。</h2><p>答：传引用是指传递的是地址而不是值本身，传值则是传递值的一份拷贝。</p>
<h2 id="003-如果要重写一个对象的equals方法，还要考虑什么？"><a href="#003-如果要重写一个对象的equals方法，还要考虑什么？" class="headerlink" title="003 如果要重写一个对象的equals方法，还要考虑什么？"></a>003 如果要重写一个对象的equals方法，还要考虑什么？</h2><p>答：hashCode。</p>
<h2 id="004-Java的”一次编写，处处运行”是如何实现的？"><a href="#004-Java的”一次编写，处处运行”是如何实现的？" class="headerlink" title="004 Java的”一次编写，处处运行”是如何实现的？"></a>004 Java的”一次编写，处处运行”是如何实现的？</h2><p>答：Java程序会被编译成字节码，这些字节码可以运行在任何平台。</p>
<h2 id="005-说明public-static-void-main-String-args-这段声明里每个关键字的作用。"><a href="#005-说明public-static-void-main-String-args-这段声明里每个关键字的作用。" class="headerlink" title="005 说明public static void main(String args[])这段声明里每个关键字的作用。"></a>005 说明<code>public static void main(String args[])</code>这段声明里每个关键字的作用。</h2><p>答：</p>
<ul>
<li>public: main方法是Java程序运行时调用的第一个方法，因此它必须对Java环境可见，所以可见性设置为pulic；</li>
<li>static: Java平台调用这个方法时不会创建这个类的一个实例，因此这个方法必须声明为static；</li>
<li>void: main方法没有返回值；</li>
<li>String是命令行传进参数的类型；</li>
<li>args是指命令行传进的字符串数组。</li>
</ul>
<h2 id="006-与equals的区别。"><a href="#006-与equals的区别。" class="headerlink" title="006 ==与equals的区别。"></a>006 <code>==</code>与<code>equals</code>的区别。</h2><p>答：<code>==</code>比较两个对象在内存里是不是同一个对象，就是说在内存里的存储位置一致。两个String对象存储的值是一样的，但有可能在内存里存储在不同的地方 .<br><code>==</code>比较的是引用而<code>equals</code>方法比较的是内容。<code>public boolean equals(Object obj)</code>这个方法是由Object对象提供的，可以由子类进行重写。默认的实现只有当对象和自身进行比较时才会返回<code>true</code>,这个时候和<code>==</code>是等价的。String, BitSet, Date, 和File都对equals方法进行了重写，对两个String对象 而言，值相等意味着它们包含同样的字符序列。对于基本类型的包装类来说，值相等意味着对应的基本类型的值一样。</p>
<h2 id="007-查看一下代码，写出结果。"><a href="#007-查看一下代码，写出结果。" class="headerlink" title="007 查看一下代码，写出结果。"></a>007 查看一下代码，写出结果。</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EqualsTest</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        String s1 = “abc”;</div><div class="line">        String s2 = s1;</div><div class="line">        String s5 = “abc”;</div><div class="line">        String s3 = <span class="keyword">new</span> String(”abc”);</div><div class="line">        String s4 = <span class="keyword">new</span> String(”abc”);</div><div class="line">        System.out.println(”== comparison : ” + (s1 == s5));</div><div class="line">        System.out.println(”== comparison : ” + (s1 == s2));</div><div class="line">        System.out.println(”Using equals method : ” + s1.equals(s2));</div><div class="line">        System.out.println(”== comparison : ” + s3 == s4);</div><div class="line">        System.out.println(”Using equals method : ” + s3.equals(s4));</div><div class="line">        &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>答：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">== comparison : <span class="keyword">true</span></div><div class="line">== comparison : <span class="keyword">true</span></div><div class="line">Using equals method : <span class="keyword">true</span></div><div class="line"><span class="keyword">false</span></div><div class="line">Using equals method :<span class="keyword">true</span></div></pre></td></tr></table></figure>
<h2 id="008-如果去掉了main方法的static修饰符会怎样？"><a href="#008-如果去掉了main方法的static修饰符会怎样？" class="headerlink" title="008 如果去掉了main方法的static修饰符会怎样？"></a>008 如果去掉了main方法的static修饰符会怎样？</h2><p>答：程序能正常编译。运行时会抛NoSuchMethodError异常。</p>
<h2 id="009-为什么oracle-type4驱动被称作瘦驱动？"><a href="#009-为什么oracle-type4驱动被称作瘦驱动？" class="headerlink" title="009 为什么oracle type4驱动被称作瘦驱动？"></a>009 为什么oracle type4驱动被称作瘦驱动？</h2><p>答：oracle提供了一个type 4 JDBC驱动，被称为瘦驱动。这个驱动包含了一个oracle自己完全用Java实现的一个TCP/IP的Net8的实现，因此它是平台独立的，可以在运行时由浏览器下载，不依赖任何客户端 的oracle实现。客户端连接字符串用的是TCP/IP的地址端口，而不是数据库名的tnsname。</p>
<h2 id="010-介绍一下finalize方法。"><a href="#010-介绍一下finalize方法。" class="headerlink" title="010 介绍一下finalize方法。"></a>010 介绍一下finalize方法。</h2><p>答：final: 常量声明。 finally: 处理异常。 finalize: 帮助进行垃圾回收。接口里声明的变量默认是final的。final类无法继承，也就是没有子类。这么做是出于基础类型的安全考虑，比如String和Integer。这样也使得编译器进行一些优化，更容易保证线程的安全性。final方法无法重写。final变量的值不能改变。finalize()方法在一个对象被销毁和回收前会被调用。finally,通常用于异常处理，不管有没有异常被抛出都会执行到。比如，关闭连接通常放到finally块中完成。</p>
<h2 id="011-什么是Java-API？"><a href="#011-什么是Java-API？" class="headerlink" title="011 什么是Java API？"></a>011 什么是Java API？</h2><p>答：Java API是大量软件组件的集合，它们提供了大量有用的功能，比如GUI组件。</p>
<h2 id="012-GregorianCalendar类是什么东西？"><a href="#012-GregorianCalendar类是什么东西？" class="headerlink" title="012 GregorianCalendar类是什么东西？"></a>012 GregorianCalendar类是什么东西？</h2><p>答：GregorianCalendar提供了西方传统日历的支持。</p>
<h2 id="013-ResourceBundle类是什么"><a href="#013-ResourceBundle类是什么" class="headerlink" title="013 ResourceBundle类是什么?"></a>013 ResourceBundle类是什么?</h2><p>答：ResourceBundle用来存储指定语言环境的资源，应用程序可以根据运行时的语言环境来加载这些资源，从而提供不同语言的展示。</p>
<h2 id="014-为什么Java里没有全局变量"><a href="#014-为什么Java里没有全局变量" class="headerlink" title="014 为什么Java里没有全局变量?"></a>014 为什么Java里没有全局变量?</h2><p>答：全局变量是全局可见的，Java不支持全局可见的变量。因为全局变量破坏了引用透明性原则，导致了命名空间的冲突。</p>
<h2 id="015-如何将String类型转化成Number类型？"><a href="#015-如何将String类型转化成Number类型？" class="headerlink" title="015 如何将String类型转化成Number类型？"></a>015 如何将String类型转化成Number类型？</h2><p>答：Integer类的valueOf方法可以将String转成Number。下面是代码示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">String numString = “<span class="number">1000</span>″;</div><div class="line"><span class="keyword">int</span> id=Integer.valueOf(numString).intValue();</div></pre></td></tr></table></figure>
<h2 id="016-SimpleTimeZone类是什么"><a href="#016-SimpleTimeZone类是什么" class="headerlink" title="016 SimpleTimeZone类是什么?"></a>016 SimpleTimeZone类是什么?</h2><p>答：SimpleTimeZone提供公历日期支持。</p>
<h2 id="017-while循环和do循环有什么不同？"><a href="#017-while循环和do循环有什么不同？" class="headerlink" title="017 while循环和do循环有什么不同？"></a>017 while循环和do循环有什么不同？</h2><p>答：while结构在循环的开始判断下一个迭代是否应该继续。do/while结构在循环的结尾来判断是否将继续下一轮迭代。do结构至少会执行一次循环体。</p>
<h2 id="018-Locale类是什么？"><a href="#018-Locale类是什么？" class="headerlink" title="018 Locale类是什么？"></a>018 Locale类是什么？</h2><p>答：Locale类用来根据语言环境来动态调整程序的输出。</p>
<h2 id="019-面向对象编程的原则是什么"><a href="#019-面向对象编程的原则是什么" class="headerlink" title="019 面向对象编程的原则是什么?"></a>019 面向对象编程的原则是什么?</h2><p>答：多态，继承和封装。</p>
<h2 id="020-介绍下继承的原则。"><a href="#020-介绍下继承的原则。" class="headerlink" title="020 介绍下继承的原则。"></a>020 介绍下继承的原则。</h2><p>答：继承使得一个对象可以获取另一个对象的属性。使用继承可以让已经测试完备的功能得以复用，并且可以一次修改，所有继承的地方都同时生效。</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[SVM思维导图]]></title>
      <url>/2018/03/15/SVM%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
      <content type="html"><![CDATA[<blockquote>
<p>来自维基百科：<br>支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。</p>
</blockquote>
<iframe src="https://my.mindnode.com/NVybfNkEyWQZ7GKoWw2TzJyKY5S8229NDvmDbpza/em#-49,-28,2" frameborder="0" marginheight="0" marginwidth="0" style="border: 1px solid rgb(204, 204, 204); width: 800px; height: 400px;" onmousewheel=""></iframe>


]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[访问服务器端的jupyter_notebook]]></title>
      <url>/2018/03/12/%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E7%9A%84jupyter-notebook/</url>
      <content type="html"><![CDATA[<p>jupyter notebook是基于ipython开发的网页形式的shell交互界面。它不仅支持Python代码运行，还支持markdown文本的编写。因此，对于开发人员而言，它可以快速记录代码或者算法的灵感，快速测试，记录文本文字的思路。<br>但有些时候会遇到Python的代码环境不完善，或者出差时临时没有办法得到完整的开发环境，这时可以利用本篇文章的方法来获得Python的开发环境，仅仅只需网络。<br><img src="https://s1.ax1x.com/2018/03/15/94zsJS.jpg" alt="94zsJS.jpg"></p>
<h2 id="所需"><a href="#所需" class="headerlink" title="所需"></a>所需</h2><ul>
<li>一台服务器（自带公网ip）</li>
<li>一台远程访问的电脑</li>
</ul>
<h2 id="安装jupyter-notebook"><a href="#安装jupyter-notebook" class="headerlink" title="安装jupyter notebook"></a>安装jupyter notebook</h2><p>一般来说，jupyter notebook是打包在anaconda的包内的。我们也是利用jupyter来开发Python的一些科学计算、数学统计。所有我们需要首先安装anaconda。</p>
<p>这是anaconda的安装地址：</p>
<ul>
<li><p>第一个是清华大学镜像站的安装地址（国内推荐）<br><code>https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.1.0-Linux-x86_64.sh</code></p>
</li>
<li><p>这是官方下载地址<br><code>https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh</code></p>
</li>
</ul>
<h3 id="远程登录服务器后，利用如下命令安装"><a href="#远程登录服务器后，利用如下命令安装" class="headerlink" title="远程登录服务器后，利用如下命令安装"></a>远程登录服务器后，利用如下命令安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -O https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.1.0-Linux-x86_64.sh</div></pre></td></tr></table></figure>
<p>在安装过程中，可以按照默认设置一路yes|enter。</p>
<h2 id="设置远程访问"><a href="#设置远程访问" class="headerlink" title="设置远程访问"></a>设置远程访问</h2><p>在安装完成后，先退出远程登录，然后再重新登录服务器。输入一下命令<code>conda --version</code>。如果结果是<code>conda 4.4.10</code>，则代表安装成功。</p>
<ul>
<li><p>生成配置jupyter的文件，命令<code>jupyter notebook --generate-config</code>：<br>  结果如下：<code>Writing default config to: /root/.jupyter/jupyter_notebook_config.py</code></p>
</li>
<li><p>设置远程登录的密码<br>  首先，<code>ipython</code>进入Python交互界面<br>  按如下代码生成密码。输入自定义的密码，然后Python会返回sha1的校验码，是服务端校验的凭证。<br>  密码需要自己记住，而<em>引号内的校验码</em>在后面需要用到，可以先复制下来。</p>
</li>
</ul>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">root@zhangssr:~# ipython</div><div class="line">Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) </div><div class="line">Type 'copyright', 'credits' or 'license' for more information</div><div class="line">IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.</div><div class="line"></div><div class="line">In [1]: from IPython.lib import passwd</div><div class="line"></div><div class="line">In [2]: passwd()</div><div class="line">Enter password: </div><div class="line">Verify password: </div><div class="line">Out[2]: 'sha1:132951527cfc:d5819e232a6e14ebaa2edd4fc93066a514fe5d9d'</div></pre></td></tr></table></figure>
</code></pre><ul>
<li><p>配置之前生成的jupyter配置文件</p>
<p>  使用vim编辑配置文件。命令为：<code>vim ~/.jupyter/jupyter_notebook_config.py</code>。</p>
<p>  vim后面的路径需要和前面的配置文件地址相同。</p>
</li>
<li><p>在文件内添加配置命令<br>  在配置文件内添加如下代码</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">c.NotebookApp.ip='输入服务器的公网ip'</div><div class="line">c.NotebookApp.password = u'刚才复制的那个sha1校验码'</div><div class="line">c.NotebookApp.open_browser = False </div><div class="line">c.NotebookApp.port =8888 #随便指定一个端口</div></pre></td></tr></table></figure>
</li>
</ul>
<p>当然，配置文件内有着许多可以配置的地方，你可以选择一些需要的地方修改配置。</p>
<p>例如，我修改了jupyter的网页打开的默认文件地址。<br>代码是：<code>c.NotebookApp.notebook_dir = &#39;/root/anaconda3/workspace&#39;</code></p>
<h2 id="启动jupyter"><a href="#启动jupyter" class="headerlink" title="启动jupyter"></a>启动jupyter</h2><p>使用命令<code>jupyter notebook --allow-root</code>启动jupyter。<em>请勿关闭jupyter的进程。</em></p>
<p><img src="https://s1.ax1x.com/2018/03/12/9fcEQI.jpg" alt="9fcEQI.jpg"></p>
<h2 id="浏览器访问"><a href="#浏览器访问" class="headerlink" title="浏览器访问"></a>浏览器访问</h2><p>在浏览器输入<code>ip地址+:+端口</code>,显示如下界面：</p>
<p><img src="https://s1.ax1x.com/2018/03/12/9fcmef.jpg" alt="9fcmef.jpg"></p>
<p>在界面输入最开始设置的密码（不是校验码）。然后登录：</p>
<p><img src="https://s1.ax1x.com/2018/03/12/9fcuTS.jpg" alt="9fcuTS.jpg"></p>
<h2 id="值得注意的地方"><a href="#值得注意的地方" class="headerlink" title="值得注意的地方"></a>值得注意的地方</h2><ol>
<li><p>网络上文章非法转载泛滥，一些文章错误百出，错的地方还都一样。比如本篇博客的<code>c.NotebookApp.password</code>网上就出现千奇八怪的错误。有些博客将前后的校验码开始的<code>sha1</code>变成了<code>sha</code>，没有了<code>1</code>。正确的是将前面的<strong>引号内的所有东西都复制</strong>，即以<code>sha1</code>开始的校验码完整复制到<code>c.NotebookApp.password</code>后面的引号内。引号前的<code>u</code>指的是utf8格式。也是必要的。</p>
</li>
<li><p>如果服务器的端口有冲突的话，<strong>jupyter会自动运行至其他端口</strong>。远程访问服务器时，启动jupyter后，会有显示http访问地址，请以该地址和端口为准。</p>
</li>
</ol>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[测试思维导图网页展示]]></title>
      <url>/2018/03/12/%E6%B5%8B%E8%AF%95%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%E7%BD%91%E9%A1%B5%E5%B1%95%E7%A4%BA/</url>
      <content type="html"><![CDATA[<p>思维导图是一个直观有效的工具。我经常用来作为学习的总结，建立直观的知识结构，帮助自己记忆。但是网页上的展示思维导图是一件很麻烦的事情。因此，当mindnode推出了网页显示思维导图的插件，立刻上手测试了一下。</p>
<h2 id="测试效果如下"><a href="#测试效果如下" class="headerlink" title="测试效果如下"></a>测试效果如下</h2><iframe src="https://my.mindnode.com/PBfBFjBopNV9zUesqsoVyMmsP4xF2Ym5q4eKWppc/em#-330,-197,0" frameborder="0" marginheight="0" marginwidth="0" style="border: 1px solid rgb(204, 204, 204); width: 800px; height: 400px;" onmousewheel=""></iframe>

<h2 id="简单介绍一下mindnode"><a href="#简单介绍一下mindnode" class="headerlink" title="简单介绍一下mindnode"></a>简单介绍一下mindnode</h2><p><em>什么是思维导图？</em></p>
<p>脑图是帮助你展现思维和想法的一种视觉化工具，它基本是由两个部分组成的：节点和联系。顾名思义，你的每一个想法都是一个节点，这些节点之间通过线条联系在一起，形成母子或相关的联系。每一个节点可以有许多个同级别的节点，而由一个节点衍生下去可以有无穷个子节点，这就和我们大脑的思维模式一样。</p>
<p>mindnode则是Mac平台上的一款思维导图工具（也有人叫脑图）。它也推出了iOS平台的版本。</p>
<p><a href="https://imgchr.com/i/9f974S" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/03/12/9f974S.md.png" alt="9f974S.md.png"></a></p>
<h2 id="如何将思维导图导出至网页代码"><a href="#如何将思维导图导出至网页代码" class="headerlink" title="如何将思维导图导出至网页代码"></a>如何将思维导图导出至网页代码</h2><p>最新的mindnode推出了mymindnode功能，让思维导图具备了网页浏览功能。不过这个功能需要内购获得使用。</p>
<ol>
<li>完成思维导图的编写</li>
<li>点击右上角的分享图标</li>
<li>在菜单栏里选择mymindnode</li>
<li>选择上传替换</li>
<li>选择在浏览器内打开</li>
<li>在网页的右上角点击分享按键</li>
<li>点击embed</li>
<li>在新界面可以调整嵌入网页模块的大小</li>
<li>复制调整后的代码</li>
<li>在网页源码或者markdown页面粘贴上述代码即可</li>
</ol>
<p>以下是流程的GIF：</p>
<p><img src="https://s1.ax1x.com/2018/03/12/9fCHVx.gif" alt="9fCHVx.gif"></p>
<h2 id="不同平台的测试结果"><a href="#不同平台的测试结果" class="headerlink" title="不同平台的测试结果"></a>不同平台的测试结果</h2><p>由于各个平台，各个浏览器的内核都不尽相同，导致iframe的框架支持并不是很好。</p>
<ul>
<li>总结来说，Chrome内核的浏览器是支持的比较好的，没有发现什么问题。</li>
<li>在安卓手机上，iframe加载的特别慢，但显示是没有问题的。</li>
<li>苹果的Safari浏览器加载也未发现问题。</li>
</ul>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[小米路由器安装科学上网插件]]></title>
      <url>/2018/01/01/%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8%E5%AE%89%E8%A3%85%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E6%8F%92%E4%BB%B6/</url>
      <content type="html"><![CDATA[<p>本篇文章讲述如何让小米路由器实现ss/ssr的方法进行科学上网。</p>
<h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><ul>
<li>小米路由器刷入开发版</li>
<li>小米路由器获得ssh权限（可以远程ssh登录小米路由器）</li>
</ul>
<p>参考文章：<br><a href="http://bbs.xiaomi.cn/t-13320370" target="_blank" rel="external">小米路由器怎么刷成开发版的</a><br><a href="http://bbs.xiaomi.cn/t-10437841" target="_blank" rel="external">达人 教你开启SSH，玩转小米路由器（获取最高权限）</a></p>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><p>本篇文章利用了Misstar Tools这个小米路由器的工具箱插件来实现科学上网。而根据Misstar Tools的官方声明，<strong>最新的小米路由器开发版</strong>会导致Misstar Tools插件<strong>无法正常工作</strong>。<br>（来自 Misstar Tools工具箱2017-12-21关于新固件的重要说明：<font color="#FF0000">重要！！！最新版固件发现一个配置文件被加密了，会导致工具箱无法打开。大家收到新通知之前不要升级固件，记得关闭自动更新！</font>）</p>
<p>因此，如果出现安装完成后，在小米路由器的管理页面无法打开Misstar Tools的界面;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">No page is registered at &apos;/web/misstar/index&apos;.</div><div class="line">If this url belongs to an extension, make sure it is properly installed.</div><div class="line">If the extension was recently installed, try removing the /tmp/luci-indexcache file.</div></pre></td></tr></table></figure>
<p>如果网页出现☝上面的内容，请手动刷回之前的开发版ROM包。</p>
<hr>
<p>如果安装完成好可以正常使用，但过了几天后小米WiFi网页后台的Misstar Tools工具箱图标消失了—–<strong>小米路由器rom自动更新导致bug</strong>。</p>
<p>请刷回老版本，关闭自动更新。重新安装Misstar Tools工具箱。（原来安装在路由器内的Misstar Tools工具箱会有些残留，会导致一些安装问题：比如<strong>无法安装在路由器内，只能安装在外置U盘。</strong>）</p>
<h2 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h2><h3 id="1-SSH登录"><a href="#1-SSH登录" class="headerlink" title="1. SSH登录"></a>1. SSH登录</h3><p><a href="https://imgchr.com/i/pF40oQ" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/01/04/pF40oQ.md.jpg" alt="pF40oQ.md.jpg"></a><br>由于小编用的是Mac系统，terminal可以直接ssh登录。Windows自行下载putty等软件登录。<br>注意：默认ssh登录密码在<a href="http://www1.miwifi.com/miwifi_open.html" target="_blank" rel="external">miwifi官网</a>上ssh工具页面中，用小米账号登录。</p>
<hr>
<p><a href="https://imgchr.com/i/pF4cQ0" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/01/04/pF4cQ0.md.jpg" alt="pF4cQ0.md.jpg"></a><br>输入默认密码后，可以看到👆的图片。</p>
<h3 id="2-执行命令"><a href="#2-执行命令" class="headerlink" title="2. 执行命令"></a>2. 执行命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://www.misstar.com/tools/appstore/install.sh -O /tmp/install.sh &amp;&amp; chmod +x /tmp/install.sh &amp;&amp; /tmp/install.sh</div></pre></td></tr></table></figure>
<p>执行上述命令后，路由器将自动安装完成Misstar Tools这个插件。</p>
<hr>
<p>卸载命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://www.misstar.com/tools/uninstall.sh -O /tmp/uninstall.sh &amp;&amp; chmod +x /tmp/uninstall.sh &amp;&amp; /tmp/uninstall.sh</div></pre></td></tr></table></figure>
<h3 id="3-科学上网插件网页安装"><a href="#3-科学上网插件网页安装" class="headerlink" title="3. 科学上网插件网页安装"></a>3. 科学上网插件网页安装</h3><p>由于一些官方原因，Misstar Tools的作者不在MT工具箱的管理页面显示科学上网的插件。但是，作者的服务器依然存在这个插件。只要修改一下网页代码，我们就可以下载这个插件到路由器。</p>
<h4 id="1-打开网页上的MT工具箱"><a href="#1-打开网页上的MT工具箱" class="headerlink" title="1.打开网页上的MT工具箱"></a>1.打开网页上的MT工具箱</h4><p><a href="https://imgchr.com/i/pA43W9" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/01/05/pA43W9.md.jpg" alt="pA43W9.md.jpg"></a><br>我们仔细查找，发现是找不到科学上网的。但我们还是有解决方法的。</p>
<h4 id="2-打开开发者工具"><a href="#2-打开开发者工具" class="headerlink" title="2.打开开发者工具"></a>2.打开开发者工具</h4><p><a href="https://imgchr.com/i/pA42ef" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/01/05/pA42ef.md.png" alt="pA42ef.md.png"></a></p>
<h4 id="3-查找aliddns，修改为ss"><a href="#3-查找aliddns，修改为ss" class="headerlink" title="3.查找aliddns，修改为ss"></a>3.查找aliddns，修改为ss</h4><p><img src="https://s1.ax1x.com/2018/01/05/pAIP3j.gif" alt="pAIP3j.gif"></p>
<p>在网页代码内<code>command+F</code>(windows是control+F)搜索aliddns,正常情况下我们可以找到三个结果。我们要的是最后一个。代码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"javascript:;"</span> <span class="attr">style</span>=<span class="string">"top: 15px;"</span> <span class="attr">class</span>=<span class="string">"layui-btn layui-btn-big"</span> <span class="attr">id</span>=<span class="string">"aliddns"</span> <span class="attr">data-type</span>=<span class="string">"add"</span>&gt;</span></div></pre></td></tr></table></figure>
<p>我们要修改上面代码中 <code>id=&quot;aliddns&quot;</code>,将aliddns换成ss。然后，点击左边aliddns的安装按钮，就可以自动完成安装。</p>
<h4 id="4-成功的结果"><a href="#4-成功的结果" class="headerlink" title="4.成功的结果"></a>4.成功的结果</h4><p><a href="https://imgchr.com/i/pAT0Bt" target="_blank" rel="external"><img src="https://s1.ax1x.com/2018/01/05/pAT0Bt.md.jpg" alt="pAT0Bt.md.jpg"></a></p>
<hr>
<p>参考博客：<a href="http://vipiu.net/archives/2017/12/29/211.html" target="_blank" rel="external">Misstar Tools工具箱安装科学上网插件</a></p>
<h3 id="4-配置ss-ssr"><a href="#4-配置ss-ssr" class="headerlink" title="4. 配置ss/ssr"></a>4. 配置ss/ssr</h3><p>其实配置ss或者ssr并没有什么好讲的。首先你得有一个ss/ssr的账号。然后照着账号信息依次填入服务器地址、端口地址、密码、加密协议、混淆等等。</p>
<p><img src="https://s1.ax1x.com/2018/01/05/pAbucQ.gif" alt="pAbucQ.gif"></p>
<h2 id="参考综合"><a href="#参考综合" class="headerlink" title="参考综合"></a>参考综合</h2><p>1.<a href="http://bbs.xiaomi.cn/t-13320370" target="_blank" rel="external">小米路由器怎么刷成开发版的</a><br>2.<a href="http://bbs.xiaomi.cn/t-10437841" target="_blank" rel="external">达人 教你开启SSH，玩转小米路由器（获取最高权限）</a><br>3.<a href="http://www1.miwifi.com/miwifi_open.html" target="_blank" rel="external">miwifi官网</a><br>4.<a href="http://vipiu.net/archives/2017/12/29/211.html" target="_blank" rel="external">Misstar Tools工具箱安装科学上网插件</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac平台的阿福管家--Alfred]]></title>
      <url>/2017/12/24/Mac%E5%B9%B3%E5%8F%B0%E7%9A%84%E9%98%BF%E7%A6%8F%E7%AE%A1%E5%AE%B6-Alfred/</url>
      <content type="html"><![CDATA[<p>Alfred是类似于Mac自带的spotlight的快速搜索打开工具，是一款效率神器，蝙蝠侠身边的阿福管家。</p>
<h2 id="官方介绍"><a href="#官方介绍" class="headerlink" title="官方介绍"></a>官方介绍</h2><p>官网地址：<a href="https://www.alfredapp.com/" target="_blank" rel="external">https://www.alfredapp.com/</a></p>
<h2 id=""><a href="#" class="headerlink" title=""></a><a href="https://imgchr.com/i/vav4K" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vav4K.md.png" alt="vav4K.md.png"></a></h2><h3 id="1-查找和浏览"><a href="#1-查找和浏览" class="headerlink" title="1.查找和浏览"></a>1.查找和浏览</h3><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://s1.ax1x.com/2017/12/24/vdPud.jpg" alt="vdPud.jpg"></h2><h3 id="2-输入更少，输出更多"><a href="#2-输入更少，输出更多" class="headerlink" title="2.输入更少，输出更多"></a>2.输入更少，输出更多</h3><h2 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="https://s1.ax1x.com/2017/12/24/vdFHI.jpg" alt="vdFHI.jpg"></h2><h3 id="3-可扩展性强，自动化程度高"><a href="#3-可扩展性强，自动化程度高" class="headerlink" title="3.可扩展性强，自动化程度高"></a>3.可扩展性强，自动化程度高</h3><p><img src="https://s1.ax1x.com/2017/12/24/vdAEt.jpg" alt="vdAEt.jpg"></p>
<h3 id="4-控制音乐"><a href="#4-控制音乐" class="headerlink" title="4.控制音乐"></a>4.控制音乐</h3><p><img src="https://s1.ax1x.com/2017/12/24/vdEUP.jpg" alt="vdEUP.jpg"></p>
<hr>
<h3 id="5-快速打开"><a href="#5-快速打开" class="headerlink" title="5.快速打开"></a>5.快速打开</h3><p>Alfred第一个令人兴奋的地方是它的热键支持双击，也就是说双击任何一个按键都可以打开Alfred的搜索框。<br><a href="https://imgchr.com/i/vd8U0" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vd8U0.md.jpg" alt="vd8U0.md.jpg"></a><br>就像上面的图片，我设置了双击command打开Alfred。还不夸张地说，打开Alfred的过程如丝般顺滑。 </p>
<h2 id="介绍一下常用的功能"><a href="#介绍一下常用的功能" class="headerlink" title="介绍一下常用的功能"></a>介绍一下常用的功能</h2><h3 id="1-打开Mac应用"><a href="#1-打开Mac应用" class="headerlink" title="1.打开Mac应用"></a>1.打开Mac应用</h3><p>直接输入应用名称，回车打开。即使没有输完整，tab键可以直接向下补全。也可以command+1/2/3等打开。<br><img src="https://s1.ax1x.com/2017/12/24/vdYCT.png" alt="vdYCT.png"></p>
<h3 id="2-搜索和打开文件"><a href="#2-搜索和打开文件" class="headerlink" title="2.搜索和打开文件"></a>2.搜索和打开文件</h3><h4 id="搜索：find-要搜索的文件名-回车在finder中打开"><a href="#搜索：find-要搜索的文件名-回车在finder中打开" class="headerlink" title="搜索：find + 要搜索的文件名;回车在finder中打开"></a>搜索：find + 要搜索的文件名;回车在finder中打开</h4><p><img src="https://s1.ax1x.com/2017/12/24/vdN2F.png" alt="vdN2F.png"></p>
<h4 id="打开：open-要打开的文件；回车直接打开文件"><a href="#打开：open-要打开的文件；回车直接打开文件" class="headerlink" title="打开：open + 要打开的文件；回车直接打开文件"></a>打开：open + 要打开的文件；回车直接打开文件</h4><p><img src="https://s1.ax1x.com/2017/12/24/vd0bR.png" alt="vd0bR.png"></p>
<h3 id="3-网页搜索"><a href="#3-网页搜索" class="headerlink" title="3.网页搜索"></a>3.网页搜索</h3><p>网页搜索支持直接谷歌搜索、亚马逊搜索、YouTube搜索、Twitter搜索、wiki搜索等等。默认的网页搜索主要支持国外的搜索引擎，但Alfred支持定制搜索。<br>    <a href="https://imgchr.com/i/vdrUx" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vdrUx.md.jpg" alt="vdrUx.md.jpg"></a></p>
<h4 id="添加百度搜索"><a href="#添加百度搜索" class="headerlink" title="添加百度搜索"></a>添加百度搜索</h4><p>在浏览器里使用百度搜索，随便搜索任意字符，复制搜索结果的网页链接:<code>https://www.baidu.com/s?wd=a&amp;tn=98012088_5_dg&amp;ch=11</code><br>            <a href="https://imgchr.com/i/vdOMQ" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vdOMQ.md.png" alt="vdOMQ.md.png"></a></p>
<p>1.将搜索的内容替换为{query}，并复制。<code>https://www.baidu.com/s?wd={query}&amp;tn=98012088_5_dg&amp;ch=11</code>。</p>
<p>2.点击Alfred设置界面内，web search项内的右下角–Add custom search，新建一个搜索项。<br><img src="https://s1.ax1x.com/2017/12/24/vdzaq.jpg" alt="vdzaq.jpg"><br>3.粘贴之前的链接到 search URL<br>4.添加标题：百度一下 {query}<br>5.添加关键词：bd<br>6.右边灰色的框内可以添加图标，直接拖动图标文件至框内即可。（图标可以不添加，不影响操作）<br><img src="https://s1.ax1x.com/2017/12/24/vwSI0.jpg" alt="vwSI0.jpg"></p>
<p>7.点击save保存。<br>8.测试一下<br><a href="https://imgchr.com/i/vwrLj" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vwrLj.md.gif" alt="vwrLj.md.gif"></a></p>
<h3 id="4-搜索浏览器内书签"><a href="#4-搜索浏览器内书签" class="headerlink" title="4.搜索浏览器内书签"></a>4.搜索浏览器内书签</h3><p><a href="https://imgchr.com/i/vwyes" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vwyes.md.jpg" alt="vwyes.md.jpg"></a></p>
<p>sources下，点击Safari或Google Chrome可开启书签搜索功能。</p>
<h3 id="5-剪切板扩展"><a href="#5-剪切板扩展" class="headerlink" title="5.剪切板扩展"></a>5.剪切板扩展</h3><p><a href="https://imgchr.com/i/vwRYV" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vwRYV.md.jpg" alt="vwRYV.md.jpg"></a></p>
<ul>
<li>保留剪切板的历史</li>
<li>热键打开，我设置了双击shift打开。<br><img src="https://s1.ax1x.com/2017/12/24/vwWWT.png" alt="vwWWT.png"></li>
</ul>
<h3 id="6-workflow扩展"><a href="#6-workflow扩展" class="headerlink" title="6.workflow扩展"></a>6.workflow扩展</h3><p>workflow是自动化的脚本，可以实现很多功能。这些workflow主要是第三方开发者所写，免费供大家使用。但Alfred要付费购买powerpack，才可以使用workflow功能。</p>
<h4 id="workflow资源站："><a href="#workflow资源站：" class="headerlink" title="workflow资源站："></a>workflow资源站：</h4><ul>
<li>官方推荐：<a href="http://www.packal.org/" target="_blank" rel="external">http://www.packal.org/</a></li>
<li>GitHub：<a href="https://github.com/zenorocha/alfred-workflows" target="_blank" rel="external">https://github.com/zenorocha/alfred-workflows</a></li>
</ul>
<h4 id="我常用的workflow"><a href="#我常用的workflow" class="headerlink" title="我常用的workflow"></a>我常用的workflow</h4><ul>
<li>Faker (v1.0.0). <a href="https://github.com/zenorocha/alfred-workflows/raw/master/faker/faker.alfredworkflow" target="_blank" rel="external">Download</a></li>
</ul>
<p><a href="https://imgchr.com/i/vw5y4" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vw5y4.md.png" alt="vw5y4.md.png"></a></p>
<ul>
<li>StrongPassword. <a href="https://raw.githubusercontent.com/vitorgalvao/alfred-workflows/master/StrongPassword/StrongPassword.alfredworkflow" target="_blank" rel="external">Download</a>.可以选择不带符号和带符号两种随机生成密码的模式。</li>
</ul>
<p><a href="https://imgchr.com/i/vwIOJ" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vwIOJ.md.png" alt="vwIOJ.md.png"></a></p>
<p><a href="https://imgchr.com/i/vwTm9" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/12/24/vwTm9.md.png" alt="vwTm9.md.png"></a></p>
]]></content>
      
        <categories>
            
            <category> 应用推荐 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[RebuildBlog]]></title>
      <url>/2017/12/23/RebuildBlog/</url>
      <content type="html"><![CDATA[<p>本篇文章就讲如何备份部署在Mac上的采用hexo和GitHub page的备份。</p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>Mac系统虽然稳定，但他的沙盒机制导致系统存在大量无用文件，而且很难清理。最重要的原因，120G硬盘太小了。<br>最终，我选择了重装系统。</p>
<h2 id="如何备份"><a href="#如何备份" class="headerlink" title="如何备份"></a>如何备份</h2><ol>
<li><p>将部署在Mac上的博客文件夹blog整个备份。官方默认是blog，我也是这么命名的。</p>
</li>
<li><p>重新安装hexo。</p>
<ul>
<li>下载安装Node.js。官网地址：<a href="https://nodejs.org/en/" target="_blank" rel="external">https://nodejs.org/en/</a></li>
<li>命令行安装hexo。<code>sudo npm install -g hexo-cli</code></li>
<li>命令行创建博客目录：<code>hexo init blog</code></li>
<li>跳转至博客目录：<code>cd blog</code></li>
<li><p>安装npm：<code>npm install</code></p>
<p>详见<a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="external">官网链接</a></p>
</li>
</ul>
</li>
<li><p>将原有的备份拷贝至blog目录，选择全部替换。</p>
</li>
<li><p>测试一下博客本地是否可用。</p>
<ul>
<li>执行命令：<code>hexo server</code> </li>
<li><p>显示如下为正常：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">INFO  Start processing</div><div class="line">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>重新链接GitHub。</p>
<ul>
<li>在blog路径下执行：<code>hexo deploy</code></li>
<li>命令行要求输入GitHub用户名：<code>Username for &#39;https://github.com&#39;:</code></li>
<li>然后输入GitHub的密码。</li>
<li>出现以下为部署成功：<code>INFO  Deploy done: git</code></li>
</ul>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 建站问题 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[pytorch_cifar10_cnn]]></title>
      <url>/2017/11/30/pytorch-cifar10-cnn/</url>
      <content type="html"><![CDATA[<p>本篇文章是pytorch的教程，利用了CIFAR10数据集和LeNet，实现了对图片的多分类。</p>
<h2 id="第一步：加载和归一化CIFAR10"><a href="#第一步：加载和归一化CIFAR10" class="headerlink" title="第一步：加载和归一化CIFAR10"></a>第一步：加载和归一化CIFAR10</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div></pre></td></tr></table></figure>
<p>torchvison可以自动加载CIFAR10和其他数据集。</p>
<h2 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h2><p><em>torchvison的输出数据集是PILIamge，它的分布是[0,1]。因此，必须把它转换成分布[-1,1]的张量。</em></p>
<ul>
<li><p>transforms.Conmpose()</p>
<ul>
<li>transforms.ToTensor()。<br>ToTensor是指把PIL.Image(RGB) 或者numpy.ndarray(H x W x C) 从0到255的值映射到0到1的范围内，并转化成Tensor格式。</li>
<li><p>transforms.Normalize()。Normalize(mean，std)是通过下面公式实现数据归一化。</p>
<font color="red">channel=（channel-mean）/std</font>



</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">transform = transforms.Compose(</div><div class="line">    [transforms.ToTensor(), </div><div class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>),(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</div><div class="line"></div><div class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>, transform=transform)</div><div class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</div><div class="line"><span class="comment">#download参数代表是否自动从官网下载CIFAR10，建议自行下载解压至当前目录的data目录下</span></div><div class="line"></div><div class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>,train=<span class="keyword">False</span>, download=<span class="keyword">True</span>, transform=transform)</div><div class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</div><div class="line">classes = (<span class="string">'plane'</span>,<span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,<span class="string">'deer'</span>,<span class="string">'dog'</span>,<span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>,<span class="string">'truck'</span>)</div><div class="line"><span class="comment">#classes是数据集标签的顺序，标签从0-9，依次对应了元组内的物体</span></div></pre></td></tr></table></figure>
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre><h2 id="关于CIFAR10"><a href="#关于CIFAR10" class="headerlink" title="关于CIFAR10"></a>关于CIFAR10</h2><ul>
<li>训练集：50000张图片</li>
<li>测试集：10000张图片</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(len(trainset))</div><div class="line">print(len(trainloader))</div><div class="line">print(len(testset))</div><div class="line">print(len(testloader))</div></pre></td></tr></table></figure>
<pre><code>50000
12500
10000
2500
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></div><div class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span></div><div class="line">    npimg = img.numpy()</div><div class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</div><div class="line">    plt.show()</div><div class="line">    <span class="comment">#如果缺少上一行代码，无法显示图片，官方是没有这一行代码的</span></div><div class="line">dataiter = iter(trainloader)</div><div class="line">images, labels = dataiter.next()</div><div class="line"></div><div class="line">imshow(torchvision.utils.make_grid(images))</div><div class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span>% classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</div></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/4e728" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/30/4e728.png" alt="4e728.png"></a></p>
<pre><code>deer  frog   dog  ship
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x = np.ones((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</div><div class="line">print(x)</div><div class="line">a = np.transpose(x, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))</div><div class="line">print(a.shape)</div></pre></td></tr></table></figure>
<pre><code>[[[ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]]

 [[ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]]

 [[ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]
  [ 1.  1.  1.  1.  1.]]]
(4, 5, 3)
</code></pre><h2 id="第二步：定义一个卷积神经网络"><a href="#第二步：定义一个卷积神经网络" class="headerlink" title="第二步：定义一个卷积神经网络"></a>第二步：定义一个卷积神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>)</div><div class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></div><div class="line">        x = self.pool(F.relu(self.conv1(x)))</div><div class="line">        x = self.pool(F.relu(self.conv2(x)))</div><div class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>)</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x= self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line">    </div><div class="line">net = Net()</div></pre></td></tr></table></figure>
<h2 id="第三步：定义损失函数和梯度下降"><a href="#第三步：定义损失函数和梯度下降" class="headerlink" title="第三步：定义损失函数和梯度下降"></a>第三步：定义损失函数和梯度下降</h2><p>如下，我们采用了交叉熵损失和动量随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"></div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>,momentum= <span class="number">0.9</span>)</div></pre></td></tr></table></figure>
<h2 id="第四步：训练网络"><a href="#第四步：训练网络" class="headerlink" title="第四步：训练网络"></a>第四步：训练网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):</div><div class="line">    running_loss = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</div><div class="line">        inputs, labels = data</div><div class="line">        </div><div class="line">        inputs, labels = Variable(inputs), Variable(labels)</div><div class="line">        optimizer.zero_grad()</div><div class="line">        </div><div class="line">        outputs = net(inputs)</div><div class="line">        loss = criterion(outputs, labels)</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line">        running_loss += loss.data[<span class="number">0</span>]</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> ==<span class="number">1999</span>:</div><div class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span>%(epoch + <span class="number">1</span>, i+<span class="number">1</span>, running_loss/<span class="number">2000</span>))</div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line">print(<span class="string">'finished training'</span>)</div></pre></td></tr></table></figure>
<pre><code>[1,  2000] loss: 2.149
[1,  4000] loss: 1.892
[1,  6000] loss: 1.683
[1,  8000] loss: 1.601
[1, 10000] loss: 1.540
[1, 12000] loss: 1.476
[2,  2000] loss: 1.419
[2,  4000] loss: 1.391
[2,  6000] loss: 1.337
[2,  8000] loss: 1.334
[2, 10000] loss: 1.327
[2, 12000] loss: 1.309
finished training
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(loss.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<pre><code>1.7544338703155518
</code></pre><h2 id="第五步：在测试数据集上测试网络"><a href="#第五步：在测试数据集上测试网络" class="headerlink" title="第五步：在测试数据集上测试网络"></a>第五步：在测试数据集上测试网络</h2><ol>
<li>先用四张图片测试一下效果</li>
<li>用10000张图片的测试测试平均分类效果</li>
<li>测试每一个种类的图片的分类效果</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dataiter = iter(trainloader)</div><div class="line">images, labels = dataiter.next()</div><div class="line"></div><div class="line">imshow(torchvision.utils.make_grid(images))</div><div class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span>% classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</div><div class="line"></div><div class="line"><span class="comment"># for label in labels:</span></div><div class="line"><span class="comment">#     print(label)</span></div></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/4eT8f" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/30/4eT8f.png" alt="4eT8f.png"></a></p>
<pre><code>plane  deer horse plane
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">outputs= net(Variable(images))</div><div class="line">_,predicted = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span>% classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</div></pre></td></tr></table></figure>
<pre><code>plane  deer horse plane
</code></pre><h3 id="2-测试一下所有测试集的分类效果"><a href="#2-测试一下所有测试集的分类效果" class="headerlink" title="2.测试一下所有测试集的分类效果"></a>2.测试一下所有测试集的分类效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">correct = <span class="number">0</span></div><div class="line">total = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</div><div class="line">    images, labels = data</div><div class="line">    outputs = net(Variable(images))</div><div class="line">    _,predicted = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line">    total += labels.size(<span class="number">0</span>) <span class="comment">#labels.size(0)的值是4，因为每次循环，输入神经网络的图片有4张</span></div><div class="line">    correct += (predicted == labels).sum()</div><div class="line">    </div><div class="line">print(<span class="string">'Accuracy of the network on the 10000 test images:%d %%'</span>%(<span class="number">100</span> * correct / total))</div></pre></td></tr></table></figure>
<pre><code>Accuracy of the network on the 10000 test images:54 %
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(total)</div><div class="line">print(correct)</div></pre></td></tr></table></figure>
<pre><code>10000
5462
</code></pre><h3 id="3-测试每一种图片的分类效果"><a href="#3-测试每一种图片的分类效果" class="headerlink" title="3. 测试每一种图片的分类效果"></a>3. 测试每一种图片的分类效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</div><div class="line"><span class="comment">#print(class_correct)</span></div><div class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</div><div class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</div><div class="line">    images, labels = data</div><div class="line">    outputs = net(Variable(images))<span class="comment">#outputs的size是torch.Size([4, 10])，表示输出是4x10的二维矩阵，4是图片数量，10是图片类别</span></div><div class="line">    _, predicted = torch.max(outputs.data, <span class="number">1</span>)<span class="comment">#torch.max(x,1)是求矩阵x在第一个维度的最大值，如果是二维矩阵，则求得是每一行的最大值</span></div><div class="line">    </div><div class="line">    c = (predicted == labels).squeeze()</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        label = labels[i]<span class="comment">#label代表了图片内的物体，用数字0-9表示物体类别,类型是int</span></div><div class="line">        class_correct[label] += c[i]</div><div class="line">        class_total[label] += <span class="number">1</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    print(<span class="string">'Accuracy of %5s : %d %%'</span>%(classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</div></pre></td></tr></table></figure>
<pre><code>Accuracy of plane : 51 %
Accuracy of   car : 65 %
Accuracy of  bird : 16 %
Accuracy of   cat : 32 %
Accuracy of  deer : 44 %
Accuracy of   dog : 56 %
Accuracy of  frog : 69 %
Accuracy of horse : 70 %
Accuracy of  ship : 72 %
Accuracy of truck : 68 %
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">outputs.size()</div></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 10])
</code></pre>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[pytorch_learing_by_LeNet]]></title>
      <url>/2017/11/28/pytorch-learing-by-LeNet/</url>
      <content type="html"><![CDATA[<p>本篇文章主要利用LeNet作为例子，来学习pytorch。</p>
<p><a href="https://imgse.com/i/d2T10" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/01/d2T10.jpg" alt="d2T10.jpg"></a></p>
<h2 id="第一步：导入相关的包"><a href="#第一步：导入相关的包" class="headerlink" title="第一步：导入相关的包"></a>第一步：导入相关的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div></pre></td></tr></table></figure>
<h2 id="第二步：构建神经网络模型"><a href="#第二步：构建神经网络模型" class="headerlink" title="第二步：构建神经网络模型"></a>第二步：构建神经网络模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(LeNet, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</div><div class="line">        </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)),(<span class="number">2</span>,<span class="number">2</span>))</div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></div><div class="line">        size = x.size()[<span class="number">1</span>:]</div><div class="line">        num_flat_features = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">            num_flat_features *= s</div><div class="line">        <span class="keyword">return</span> num_flat_features</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">net = LeNet()</div><div class="line">print(net)</div></pre></td></tr></table></figure>
<pre><code>LeNet (
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear (400 -&gt; 120)
  (fc2): Linear (120 -&gt; 84)
  (fc3): Linear (84 -&gt; 10)
)
</code></pre><h2 id="关于num-flat-features函数与x-view函数"><a href="#关于num-flat-features函数与x-view函数" class="headerlink" title="关于num_flat_features函数与x.view函数"></a>关于num_flat_features函数与x.view函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = torch.rand(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</div><div class="line">print(x)</div></pre></td></tr></table></figure>
<pre><code>(0 ,.,.) = 
  0.1522  0.8857  0.3232  0.5216  0.8370
  0.1891  0.2239  0.2778  0.2757  0.5710
  0.3799  0.4527  0.0544  0.6282  0.7652
  0.0942  0.5210  0.9468  0.2108  0.1887

(1 ,.,.) = 
  0.6245  0.1676  0.2266  0.9007  0.0337
  0.7066  0.2328  0.6476  0.5590  0.2812
  0.5691  0.8334  0.9075  0.6196  0.8155
  0.3516  0.1676  0.0758  0.3752  0.1590

(2 ,.,.) = 
  0.0487  0.2262  0.0989  0.6582  0.7160
  0.4414  0.6422  0.1275  0.3679  0.2844
  0.8888  0.2635  0.8464  0.7274  0.0975
  0.4739  0.0024  0.2554  0.8249  0.9883
[torch.FloatTensor of size 3x4x5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">size = x.size()[<span class="number">1</span>:]<span class="comment">#忽略了第一个维度，第一个维度相当于图片的深度，也可以理解为图片的数量，剩余的大小是图片的长与宽</span></div><div class="line">print(size) <span class="comment">#size是[4,5]的列表</span></div><div class="line"></div><div class="line">num_flat_feature = <span class="number">1</span></div><div class="line"><span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">    num_flat_feature *= s</div><div class="line">print(num_flat_feature)    <span class="comment"># 计算张量x的总特征量，相当于将一个三维的长方体切片分开，并重新排列在一起，每一行代表一张图片</span></div></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 5])
20
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x = x.view(<span class="number">-1</span>,num_flat_feature)<span class="comment">#view的第一个参数是-1时，仅仅支持第二个参数可以整除所有元素的数量，上述即20可以整除60</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(x) <span class="comment">#将三维矩阵转化为二维矩阵，大小是3x20</span></div></pre></td></tr></table></figure>
<pre><code>Columns 0 to 9 
 0.1522  0.8857  0.3232  0.5216  0.8370  0.1891  0.2239  0.2778  0.2757  0.5710
 0.6245  0.1676  0.2266  0.9007  0.0337  0.7066  0.2328  0.6476  0.5590  0.2812
 0.0487  0.2262  0.0989  0.6582  0.7160  0.4414  0.6422  0.1275  0.3679  0.2844

Columns 10 to 19 
 0.3799  0.4527  0.0544  0.6282  0.7652  0.0942  0.5210  0.9468  0.2108  0.1887
 0.5691  0.8334  0.9075  0.6196  0.8155  0.3516  0.1676  0.0758  0.3752  0.1590
 0.8888  0.2635  0.8464  0.7274  0.0975  0.4739  0.0024  0.2554  0.8249  0.9883
[torch.FloatTensor of size 3x20]
</code></pre><h2 id="关于模型的参数"><a href="#关于模型的参数" class="headerlink" title="关于模型的参数"></a>关于模型的参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">params = list(net.parameters()) <span class="comment">#将模型内的参数转化为列表</span></div><div class="line">print(len(params))</div><div class="line">print(params[<span class="number">0</span>].size())<span class="comment">#params[0]是第一个卷积层的参数列表</span></div></pre></td></tr></table></figure>
<pre><code>10
torch.Size([6, 1, 5, 5])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(params[<span class="number">0</span>])<span class="comment">#看看具体都是些什么</span></div></pre></td></tr></table></figure>
<pre><code>Parameter containing:
(0 ,0 ,.,.) = 
  0.0159 -0.1854 -0.1673  0.1858  0.1004
  0.0806 -0.1890  0.1770 -0.0835  0.1374
 -0.0633  0.1721  0.0248 -0.0270 -0.0597
 -0.1703  0.1079 -0.0374 -0.1265 -0.0034
  0.0776 -0.1505 -0.1946 -0.1897  0.1594

(1 ,0 ,.,.) = 
  0.1435  0.1547 -0.0932  0.1475  0.0345
  0.0012 -0.0629  0.1618  0.0432 -0.1430
  0.1472  0.0943 -0.1746  0.1368  0.0764
  0.0732 -0.0053 -0.0747 -0.1721 -0.0330
  0.1236  0.0149  0.1268 -0.1109  0.1937

(2 ,0 ,.,.) = 
  0.0848 -0.1940  0.0660  0.1426  0.1521
 -0.1374 -0.0445  0.1238 -0.1351 -0.1130
 -0.1227  0.0647 -0.0145  0.1483  0.1827
  0.1801 -0.1854  0.0648  0.0556 -0.1266
  0.1414 -0.1424  0.1032 -0.0185 -0.0612

(3 ,0 ,.,.) = 
  0.1210  0.1905  0.1715 -0.0207 -0.0109
  0.1858 -0.0217 -0.1399  0.0052  0.0112
  0.1758  0.0133 -0.0914 -0.1232 -0.0880
  0.1816 -0.0598  0.1999  0.1707 -0.1307
 -0.1647 -0.1004  0.0756 -0.0015  0.1869

(4 ,0 ,.,.) = 
  0.0781  0.1446 -0.0782 -0.1754  0.0640
 -0.1273  0.1269  0.1654  0.0301 -0.1362
 -0.0435 -0.0326  0.1833  0.1044 -0.1227
 -0.1369 -0.1593  0.1820 -0.1265 -0.0487
  0.1353  0.0620 -0.1766  0.1219  0.1566

(5 ,0 ,.,.) = 
 -0.1660  0.1400 -0.1607  0.0005 -0.1939
  0.0535  0.1525  0.0882  0.1387 -0.1725
 -0.1346 -0.0119  0.0308 -0.0602 -0.0366
  0.0327  0.0040  0.1337  0.0573  0.0842
 -0.1316  0.0293 -0.0490  0.0161 -0.0296
[torch.FloatTensor of size 6x1x5x5]
</code></pre><h2 id="第三步：前向传播"><a href="#第三步：前向传播" class="headerlink" title="第三步：前向传播"></a>第三步：前向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">input = Variable(torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>))</div><div class="line">out = net(input)</div><div class="line">print(out)</div></pre></td></tr></table></figure>
<pre><code>Variable containing:
 0.0735 -0.0718  0.1036  0.0527  0.0256  0.1827  0.0848  0.0878 -0.0959  0.1593
[torch.FloatTensor of size 1x10]
</code></pre><h2 id="系统自动反向传播"><a href="#系统自动反向传播" class="headerlink" title="系统自动反向传播"></a>系统自动反向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">net.zero_grad()<span class="comment">#随机初始化参数</span></div><div class="line">out.backward(torch.randn(<span class="number">1</span>,<span class="number">10</span>))<span class="comment">#只是示例，不包含损失函数</span></div></pre></td></tr></table></figure>
<h2 id="第四步：损失函数"><a href="#第四步：损失函数" class="headerlink" title="第四步：损失函数"></a>第四步：损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">output = net(input)</div><div class="line">target = Variable(torch.arange(<span class="number">1</span>,<span class="number">11</span>))<span class="comment">#正常神经网络的输入输出是成对的，我们这里随机产生了目标输出的值</span></div><div class="line">criterion = nn.MSELoss()</div><div class="line"></div><div class="line">loss = criterion(output, target)<span class="comment">#将目标输出的值和神经网络产生的值比较得到了loss</span></div><div class="line">print(loss)</div></pre></td></tr></table></figure>
<pre><code>Variable containing:
 37.7703
[torch.FloatTensor of size 1]
</code></pre><h2 id="第五步：反向传播"><a href="#第五步：反向传播" class="headerlink" title="第五步：反向传播"></a>第五步：反向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">net.zero_grad()</div><div class="line"></div><div class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</div><div class="line">print(net.conv1.bias.grad)</div><div class="line"></div><div class="line">loss.backward()</div><div class="line"></div><div class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</div><div class="line">print(net.conv1.bias.grad)</div></pre></td></tr></table></figure>
<pre><code>conv1.bias.grad before backward
Variable containing:
 0
 0
 0
 0
 0
 0
[torch.FloatTensor of size 6]

conv1.bias.grad after backward
Variable containing:
1.00000e-02 *
  0.0236
 -4.9294
 -4.1088
  3.7286
 -1.2244
 -0.2389
[torch.FloatTensor of size 6]
</code></pre><h2 id="第六步：更新权重"><a href="#第六步：更新权重" class="headerlink" title="第六步：更新权重"></a>第六步：更新权重</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#手动更新</span></div><div class="line">learning_rate = <span class="number">0.01</span></div><div class="line"><span class="keyword">for</span> parameter <span class="keyword">in</span> net.parameters():</div><div class="line">    parameter.data.sub_(parameter.grad.data * learning_rate)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#pytorch自带各种梯度下降的方法</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)<span class="comment">#随机梯度下降</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    optimizer.zero_grad()   </div><div class="line">    output = net(input)</div><div class="line">    loss = criterion(output, target)</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()    </div><div class="line">    <span class="comment">#由于没有使用数据集，所有的参数的值都是随机的，因此loss并没有实际意义</span></div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习模型评估指标]]></title>
      <url>/2017/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
      <content type="html"><![CDATA[<p>如果我们有几个甚至十几个模型，需要从中挑选出最佳的模型。我们需要一个统一的指标来判断模型的表现。</p>
<h2 id="查准率和查全率"><a href="#查准率和查全率" class="headerlink" title="查准率和查全率"></a>查准率和查全率</h2><ul>
<li>查准率（precision）<br>  在所有判断为🐱的样本里，有多少真正是猫。<blockquote>
<p> Precision Of all the images we predicted y=1, what fraction of it have cats?  </p>
</blockquote>
</li>
</ul>
<p><img src="https://s1.ax1x.com/2017/11/06/BupDg.png" alt="BupDg.png"></p>
<ul>
<li><p>查全率（recall）<br>  在所有🐱的样本中，有%被判断出来了。<br>  &gt;<br>  Of all the images that actually have cats, what fraction of it did we correctly identifying have cats? </p>
<p> <img src="https://s1.ax1x.com/2017/11/06/Bu9bQ.png" alt="Bu9bQ.png"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2></li>
</ul>
<table>
<thead>
<tr>
<th>classifier</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>95%</td>
<td>90%</td>
</tr>
<tr>
<td>B</td>
<td>98%</td>
<td>85%</td>
</tr>
</tbody>
</table>
<p>上述两个模型在查准率和查全率上各有优劣，直观上很难判断哪个更好。</p>
<p>因此，我们采用了<em>F1 score</em>来作为指标。</p>
<h2 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h2><p>公式：</p>
<p><a href="https://imgchr.com/i/BuSKS" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/06/BuSKS.png" alt="BuSKS.png"></a></p>
<table>
<thead>
<tr>
<th>Classifier</th>
<th>precision</th>
<th>recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>95%</td>
<td>90%</td>
<td>92.4%</td>
</tr>
<tr>
<td>B</td>
<td>98%</td>
<td>85%</td>
<td>91.0%</td>
</tr>
</tbody>
</table>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[网站插入网易云音乐_测试]]></title>
      <url>/2017/11/01/%E7%BD%91%E7%AB%99%E6%8F%92%E5%85%A5%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/</url>
      <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=443513&auto=1&height=66"></iframe>

<h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><p>在浏览器上找到想要的音乐，点击选择生成外链播放器。<br><img src="https://s1.ax1x.com/2017/11/01/dhEEF.jpg" alt="dhEEF.jpg"></p>
<h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><p>选择iframe插件，并复制框内链接。<br><img src="https://s1.ax1x.com/2017/11/01/dhkHU.jpg" alt="dhkHU.jpg"></p>
<h2 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h2><p>在markdown撰写的博文内粘贴。<br><img src="https://s1.ax1x.com/2017/11/01/dhFBT.jpg" alt="dhFBT.jpg"></p>
]]></content>
      
        <categories>
            
            <category> 建站问题 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow_CNN]]></title>
      <url>/2017/11/01/TensorFlow-CNN/</url>
      <content type="html"><![CDATA[<p>TensorFlow自带了很多内置的函数。本篇文章讲了CNN模型里卷积函数和池化函数。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h2 id=""><a href="#" class="headerlink" title=""></a><a href="https://imgchr.com/i/d2T10" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/01/d2T10.md.jpg" alt="d2T10.jpg" border="0"></a></h2><p>从上图可以看出，随着不断地卷积，图像的深度增减，图像的长宽开始减小。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.nn.conv2d?</div><div class="line"><span class="comment">#如下是TensorFlow对conv2d函数的注释</span></div></pre></td></tr></table></figure>
<p>Signature: <em>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</em></p>
<p>Docstring:<br>Computes a 2-D convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code><br>and a filter / kernel tensor of shape<br><code>[filter_height, filter_width, in_channels, out_channels]</code>, this op<br>performs the following:</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape<br><code>[filter_height * filter_width * in_channels, output_channels]</code>.</li>
<li>Extracts image patches from the input tensor to form a <em>virtual</em><br>tensor of shape <code>[batch, out_height, out_width,
filter_height * filter_width * in_channels]</code>.</li>
<li>For each patch, right-multiplies the filter matrix and the image patch<br>vector.</li>
</ol>
<p>In detail, with the default NHWC format,</p>
<pre><code>output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                    filter[di, dj, q, k]
</code></pre><p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the same<br>horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:<br>  input: A <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>.<br>    A 4-D tensor. The dimension order is interpreted according to the value<br>    of <code>data_format</code>, see below for details.</p>
<p>  filter: A <code>Tensor</code>. Must have the same type as <code>input</code>.<br>    A 4-D tensor of shape<br>    <code>[filter_height, filter_width, in_channels, out_channels]</code></p>
<p>  strides: A list of <code>ints</code>.<br>    1-D tensor of length 4.  The stride of the sliding window for each<br>    dimension of <code>input</code>. The dimension order is determined by the value of<br>      <code>data_format</code>, see below for details.</p>
<p>  padding: A <code>string</code> from: <code>&quot;SAME&quot;, &quot;VALID&quot;</code>.<br>    The type of padding algorithm to use.</p>
<p>  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.</p>
<p>  data_format: An optional <code>string</code> from: <code>&quot;NHWC&quot;, &quot;NCHW&quot;</code>. Defaults to <code>&quot;NHWC&quot;</code>.<br>    Specify the data format of the input and output data. With the<br>    default format “NHWC”, the data is stored in the order of:<br>        [batch, height, width, channels].<br>    Alternatively, the format could be “NCHW”, the data storage order of:<br>        [batch, channels, height, width].</p>
<p>  name: A name for the operation (optional).</p>
<p>Returns:<br>  A <code>Tensor</code>. Has the same type as <code>input</code>.<br>  A 4-D tensor. The dimension order is determined by the value of<br>  <code>data_format</code>, see below for details.</p>
<p>File:      /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py</p>
<p>Type:      function</p>
<hr>
<h2 id="卷积操作示意"><a href="#卷积操作示意" class="headerlink" title="卷积操作示意"></a>卷积操作示意</h2><p><img src="https://s1.ax1x.com/2017/11/01/dRugP.gif" alt="dRugP.gif"></p>
<hr>
<h2 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h2><p>卷积操作在CNN里面占主要位置。TensorFlow自带的卷积函数常见的是如上的tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</p>
<ul>
<li>input<br>  input是输入的数据，数据以张量形式输入（多维矩阵），形状是[batch, in_height, in_width, in_channels]<ul>
<li>batch为训练过程中每迭代一次迭代的照片数。</li>
<li>in_height是数据矩阵的行，如果输入数据是图片，则代表图片的高度</li>
<li>in_width是矩阵的列</li>
<li>in_channels是图片的深度</li>
</ul>
</li>
<li>filter<br>  一个Tensor，每个元素的类型和input类型一致。filter的形状：[filter_height,filter_width,in_channels,out_channels]<ul>
<li>filter_height</li>
<li>filter_width</li>
<li>in_channels是输入的图片的深度</li>
<li>out_channels是经过卷积后的图片的深度</li>
</ul>
</li>
<li>strides</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.max_pool?</div></pre></td></tr></table></figure>
<p>Signature: tf.nn.max_pool(value, ksize, strides, padding, data_format=’NHWC’, name=None)</p>
<p>Docstring:<br>Performs the max pooling on the input.</p>
<p>Args:<br>  value: A 4-D <code>Tensor</code> with shape <code>[batch, height, width, channels]</code> and<br>    type <code>tf.float32</code>.</p>
<p>  ksize: A list of ints that has length &gt;= 4.  The size of the window for<br>    each dimension of the input tensor.</p>
<p>  strides: A list of ints that has length &gt;= 4.  The stride of the sliding<br>    window for each dimension of the input tensor.</p>
<p>  padding: A string, either <code>&#39;VALID&#39;</code> or <code>&#39;SAME&#39;</code>. The padding algorithm.<br>    See the @{tf.nn.convolution$comment here}</p>
<p>  data_format: A string. ‘NHWC’ and ‘NCHW’ are supported.</p>
<p>  name: Optional name for the operation.</p>
<p>Returns:<br>  A <code>Tensor</code> with type <code>tf.float32</code>.  The max pooled output tensor.</p>
<p>File:      /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py</p>
<p>Type:      function</p>
<h2 id="max-pooling示意"><a href="#max-pooling示意" class="headerlink" title="max_pooling示意"></a>max_pooling示意</h2><p><a href="https://imgchr.com/i/d2hkj" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/11/01/d2hkj.md.jpg" alt="d2hkj.jpg" border="0"></a><br>max_pooling以类似卷积滑动的效果，从滑动窗口所在的矩阵选出最大值。上图中，滑动窗口的大小是2x2；步长是2，代表滑动的间距为2。因此，滑动过程产生的矩阵有4个。</p>
<h2 id="tf-nn-max-pool"><a href="#tf-nn-max-pool" class="headerlink" title="tf.nn.max_pool"></a>tf.nn.max_pool</h2><p>tf.nn.max_pool(value, ksize, strides, padding, data_format=’NHWC’, name=None)</p>
<ul>
<li>value是输入的张量，形状为<code>[batch, height, width, channels]</code></li>
<li>ksize是int类型的列表。通常为<code>[1, height, width, 1]</code></li>
<li>strides是int类型的列表。通常为<code>[1, stride, stride, 1]</code></li>
<li><p>padding是为了<del>防止滑动窗口的大小与原矩阵大小不符而在矩阵外围添加一层数据</del>。</p>
<p>  padding的作用是为了防止边缘的防止边缘的特征丢失（卷积时，边缘的值在滑动时仅仅出现几次，矩阵中间的值参与较多）。也有防止输出矩阵的形状缩小过快，导致特征丢失的作用。<br>  在TensorFlow中，padding默认有两种状态。</p>
<ul>
<li><code>&#39;VALID&#39;</code> ：没有padding。</li>
<li><code>&#39;SAME&#39;</code>：卷积前后的矩阵形状相同。常用选项。</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[LSTM_TensorFlow using MNIST]]></title>
      <url>/2017/10/30/LSTM-TensorFlow/</url>
      <content type="html"><![CDATA[<p>长期记忆（LSTM）是这些年人们使用的循环神经网络中最常见的类型。它们主要用于顺序数据。我们建立了一个LSTM的模型，采用mnist数据集来测试模型效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</code></pre><h2 id="数据集内容分为三类"><a href="#数据集内容分为三类" class="headerlink" title="数据集内容分为三类"></a>数据集内容分为三类</h2><ul>
<li>Training data(mnist.train)-55000 images of training data</li>
<li>Test data(mnist.test)-10000 images of test data</li>
<li>Validation data(mnist.validation)-5000 images of validation data.</li>
</ul>
<h2 id="Shape-of-the-data"><a href="#Shape-of-the-data" class="headerlink" title="Shape of the data"></a>Shape of the data</h2><p>三类数据集的形状都是一样的。</p>
<p>训练集包含55000张图片，每张图片是28x28像素的。因此，一张图片是28x28的矩阵，可以转换为784x1的向量。而所有的训练集图片组合在一起，可以构成55000x784的矩阵。</p>
<blockquote>
<p>The training set consists of 55000 images of 28 pixels X 28 pixels each.These 784(28X28) pixel values are flattened in form of a single vector of dimensionality 784.The collection of all such 55000 pixel vectors(one for each image) is stored in form of a numpy array of shape (55000,784) and is referred to as mnist.train.images.</p>
</blockquote>
<p>每张训练集的图片都关联着一个标签，这个标签代表数据集图片代表的数字（这个数据集是手写数字的图片）。由于有0-9十个数字，标签可以用10x1的向量表示。55000张图片的标签组合在一起构成了大小为（55000，10）的矩阵</p>
<blockquote>
<p>Each of these 55000 training images are associated with a label representing the class to which that image belongs.There are 10 such classes(0,1,2…9).Class labels are represented in one hot encoded form.Thus the labels are stored in form of numpy array of shape (55000,10) and is referred to as mnist.train.labels.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">time_steps = <span class="number">28</span></div><div class="line">num_units = <span class="number">128</span></div><div class="line">n_input = <span class="number">28</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">n_classes = <span class="number">10</span></div><div class="line">batch_size = <span class="number">128</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))</div><div class="line">out_bias=tf.Variable(tf.random_normal([n_classes]))</div><div class="line"></div><div class="line"><span class="comment">#defining placeholders</span></div><div class="line"><span class="comment">#input image placeholder</span></div><div class="line">x=tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,time_steps,n_input])</div><div class="line"><span class="comment">#input label placeholder</span></div><div class="line">y=tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,n_classes])</div><div class="line"></div><div class="line">input1 = tf.unstack(x, time_steps, <span class="number">1</span>)</div><div class="line"></div><div class="line">lstm_layer = rnn.BasicLSTMCell(num_units,forget_bias=<span class="number">1</span>)</div><div class="line"><span class="comment">#这里num_units指的是LSTM单元中的单位数。</span></div><div class="line"></div><div class="line">outputs,_ = rnn.static_rnn(lstm_layer,input1,dtype=tf.float32)</div></pre></td></tr></table></figure>
<h2 id="一个普通的RNN单元"><a href="#一个普通的RNN单元" class="headerlink" title="一个普通的RNN单元"></a>一个普通的RNN单元</h2><p>RNN单元由时间展开，如下：<br><a href="https://imgchr.com/i/axVTs" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/10/30/axVTs.md.png" alt="axVTs.png" border="0"></a></p>
<ul>
<li>$x_{t}$是在时间步长t的输入。</li>
<li>$s_{t}$是在时间步长t处的隐藏状态。它可以被可视化为我们网络的“内存”。</li>
<li>$Ò_吨$指的是输出在时间步长t。</li>
</ul>
<p>U，V和W是所有时间步长共享的参数。该参数共享的意义在于，我们的模型在不同输入的每个时间步长执行相同的任务。<br>我们通过展开RNN实现的是，在每个时间步骤中，网络可以被视为前馈网络，同时考虑到前一个时间步长的输出（由时间步长之间的连接表示）。</p>
<p><code>lstm_layer = rnn.BasicLSTMCell(num_units,forget_bias=1)</code><br><code>#这里num_units指的是LSTM单元中的单位数。</code><br>num_units可以解释为前馈神经网络隐藏层的类比。前馈神经网络隐层中的节点num_units数目等于网络每个时间步长的LSTM单元中LSTM单元的数量。<br><a href="https://imgchr.com/i/avqyD" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/10/30/avqyD.md.png" alt="avqyD.png" border="0"></a></p>
<hr>
<p><code>tf.static_rnn(cell,inputs)</code></p>
<p>该inputs参数接受形状张量列表[batch_size,input_size]。该列表的长度是网络展开的时间步长数，即该列表的每个元素对应于我们展开网络的相应时间步长的输入。</p>
<p>对于我们的MNIST图像的情况，我们有大小为28X28的图像。它们可以被推断为具有28行28像素的图像。我们将通过28个时间步骤展开我们的网络，使得在每个时间步长，我们可以输入一行28像素（input_size），从而通过28个时间步长完整的图像。如果我们提供batch_size图像数量，每个时间步长将提供相应的batch_size图像行。下图应该清除任何疑问 -</p>
<p><a href="https://imgchr.com/i/avbQO" target="_blank" rel="external"><img src="https://s1.ax1x.com/2017/10/30/avbQO.md.png" alt="avbQO.png" border="0"></a></p>
<p>生成的输出static_rnn是形状的张量列表[batch_size,num_units]。列表的长度是网络展开的时间步长数，即每个时间步长的一个输出张量。在这个实现中，我们将只关注最后时间的输出当图像的所有行被提供给RNN即在最后时间步长时将产生预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">prediction = tf.matmul(outputs[<span class="number">-1</span>], out_weights) + out_bias</div><div class="line"><span class="comment">#prediction = tf.matmul(outputs[-1], out_weights) + out_bias</span></div><div class="line">loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))</div><div class="line"></div><div class="line">opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)</div><div class="line"></div><div class="line">correct_prediction = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(y,<span class="number">1</span>))</div><div class="line"></div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">init=tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    iter=<span class="number">1</span></div><div class="line">    <span class="keyword">while</span> iter&lt;<span class="number">800</span>:</div><div class="line">        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)</div><div class="line"></div><div class="line">        batch_x=batch_x.reshape((batch_size,time_steps,n_input))</div><div class="line"></div><div class="line">        sess.run(opt, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">        </div><div class="line">        <span class="comment">#测试集检测</span></div><div class="line">        test_data = mnist.test.images[:<span class="number">128</span>].reshape((<span class="number">-1</span>, time_steps, n_input))</div><div class="line">        test_label = mnist.test.labels[:<span class="number">128</span>]</div><div class="line">        print(<span class="string">"Testing Accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: test_data, y: test_label&#125;))</div><div class="line">        <span class="comment">#测试集检测</span></div><div class="line">        <span class="keyword">if</span> iter %<span class="number">10</span>==<span class="number">0</span>:</div><div class="line">            acc=sess.run(accuracy,feed_dict=&#123;x:batch_x,y:batch_y&#125;)</div><div class="line">            los=sess.run(loss,feed_dict=&#123;x:batch_x,y:batch_y&#125;)</div><div class="line">            print(<span class="string">"For iter "</span>,iter)</div><div class="line">            print(<span class="string">"Accuracy "</span>,acc)</div><div class="line">            print(<span class="string">"Loss "</span>,los)</div><div class="line">            print(<span class="string">"__________________"</span>)</div><div class="line"></div><div class="line">        iter=iter+<span class="number">1</span></div></pre></td></tr></table></figure>
<pre><code>Testing Accuracy: 0.109375
Testing Accuracy: 0.109375
Testing Accuracy: 0.132812
Testing Accuracy: 0.15625
Testing Accuracy: 0.117188
Testing Accuracy: 0.21875
Testing Accuracy: 0.296875
Testing Accuracy: 0.304688
Testing Accuracy: 0.296875
Testing Accuracy: 0.304688
For iter  10
Accuracy  0.257812
Loss  2.07883
__________________
Testing Accuracy: 0.328125
Testing Accuracy: 0.359375
Testing Accuracy: 0.382812
Testing Accuracy: 0.367188
Testing Accuracy: 0.382812
Testing Accuracy: 0.375
Testing Accuracy: 0.359375
Testing Accuracy: 0.351562
Testing Accuracy: 0.359375
Testing Accuracy: 0.382812
For iter  20
Accuracy  0.445312
Loss  1.69872
__________________
Testing Accuracy: 0.382812
Testing Accuracy: 0.421875
Testing Accuracy: 0.429688
Testing Accuracy: 0.46875
Testing Accuracy: 0.476562
Testing Accuracy: 0.5
Testing Accuracy: 0.507812
Testing Accuracy: 0.5
Testing Accuracy: 0.5
Testing Accuracy: 0.507812
For iter  30
Accuracy  0.4375
Loss  1.50721
__________________
Testing Accuracy: 0.515625
Testing Accuracy: 0.539062
Testing Accuracy: 0.570312
Testing Accuracy: 0.585938
Testing Accuracy: 0.585938
Testing Accuracy: 0.59375
Testing Accuracy: 0.59375
Testing Accuracy: 0.570312
Testing Accuracy: 0.578125
Testing Accuracy: 0.578125
For iter  40
Accuracy  0.640625
Loss  0.962442
__________________
Testing Accuracy: 0.59375
Testing Accuracy: 0.554688
Testing Accuracy: 0.554688
Testing Accuracy: 0.578125
Testing Accuracy: 0.554688
Testing Accuracy: 0.59375
Testing Accuracy: 0.609375
Testing Accuracy: 0.617188
Testing Accuracy: 0.632812
Testing Accuracy: 0.640625
For iter  50
Accuracy  0.765625
Loss  0.778242
__________________
Testing Accuracy: 0.648438
Testing Accuracy: 0.640625
Testing Accuracy: 0.640625
Testing Accuracy: 0.625
Testing Accuracy: 0.648438
Testing Accuracy: 0.648438
Testing Accuracy: 0.671875
Testing Accuracy: 0.6875
Testing Accuracy: 0.695312
Testing Accuracy: 0.664062
For iter  60
Accuracy  0.679688
Loss  0.985955
__________________
Testing Accuracy: 0.648438
Testing Accuracy: 0.664062
Testing Accuracy: 0.703125
Testing Accuracy: 0.6875
Testing Accuracy: 0.703125
Testing Accuracy: 0.703125
Testing Accuracy: 0.726562
Testing Accuracy: 0.734375
Testing Accuracy: 0.757812
Testing Accuracy: 0.710938
For iter  70
Accuracy  0.820312
Loss  0.59718
__________________
Testing Accuracy: 0.695312
Testing Accuracy: 0.695312
Testing Accuracy: 0.742188
Testing Accuracy: 0.75
Testing Accuracy: 0.75
Testing Accuracy: 0.757812
Testing Accuracy: 0.757812
Testing Accuracy: 0.773438
Testing Accuracy: 0.75
Testing Accuracy: 0.742188
For iter  80
Accuracy  0.789062
Loss  0.680092
__________________
Testing Accuracy: 0.773438
Testing Accuracy: 0.789062
Testing Accuracy: 0.835938
Testing Accuracy: 0.835938
Testing Accuracy: 0.851562
Testing Accuracy: 0.867188
Testing Accuracy: 0.835938
Testing Accuracy: 0.796875
Testing Accuracy: 0.78125
Testing Accuracy: 0.796875
For iter  90
Accuracy  0.8125
Loss  0.688224
__________________
Testing Accuracy: 0.796875
Testing Accuracy: 0.835938
Testing Accuracy: 0.84375
Testing Accuracy: 0.851562
Testing Accuracy: 0.84375
Testing Accuracy: 0.835938
Testing Accuracy: 0.84375
Testing Accuracy: 0.867188
Testing Accuracy: 0.835938
Testing Accuracy: 0.835938
For iter  100
Accuracy  0.828125
Loss  0.439927
__________________
Testing Accuracy: 0.835938
Testing Accuracy: 0.867188
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.914062
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.875
Testing Accuracy: 0.882812
For iter  110
Accuracy  0.867188
Loss  0.444241
__________________
Testing Accuracy: 0.882812
Testing Accuracy: 0.882812
Testing Accuracy: 0.890625
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.90625
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.921875
For iter  120
Accuracy  0.84375
Loss  0.485538
__________________
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.90625
Testing Accuracy: 0.90625
Testing Accuracy: 0.898438
Testing Accuracy: 0.90625
Testing Accuracy: 0.921875
Testing Accuracy: 0.921875
For iter  130
Accuracy  0.90625
Loss  0.382915
__________________
Testing Accuracy: 0.914062
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.90625
Testing Accuracy: 0.929688
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.921875
For iter  140
Accuracy  0.867188
Loss  0.356146
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.914062
Testing Accuracy: 0.875
Testing Accuracy: 0.859375
Testing Accuracy: 0.914062
Testing Accuracy: 0.898438
Testing Accuracy: 0.898438
Testing Accuracy: 0.90625
For iter  150
Accuracy  0.890625
Loss  0.376553
__________________
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.890625
Testing Accuracy: 0.867188
Testing Accuracy: 0.898438
Testing Accuracy: 0.914062
Testing Accuracy: 0.90625
Testing Accuracy: 0.914062
Testing Accuracy: 0.90625
Testing Accuracy: 0.921875
For iter  160
Accuracy  0.867188
Loss  0.371748
__________________
Testing Accuracy: 0.929688
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
For iter  170
Accuracy  0.890625
Loss  0.290704
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.921875
Testing Accuracy: 0.914062
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.929688
Testing Accuracy: 0.929688
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
For iter  180
Accuracy  0.921875
Loss  0.320236
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.929688
Testing Accuracy: 0.929688
Testing Accuracy: 0.921875
Testing Accuracy: 0.921875
Testing Accuracy: 0.929688
Testing Accuracy: 0.921875
Testing Accuracy: 0.921875
Testing Accuracy: 0.921875
Testing Accuracy: 0.921875
For iter  190
Accuracy  0.953125
Loss  0.20053
__________________
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.921875
For iter  200
Accuracy  0.914062
Loss  0.305831
__________________
Testing Accuracy: 0.90625
Testing Accuracy: 0.921875
Testing Accuracy: 0.90625
Testing Accuracy: 0.90625
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.875
Testing Accuracy: 0.890625
Testing Accuracy: 0.929688
For iter  210
Accuracy  0.867188
Loss  0.356829
__________________
Testing Accuracy: 0.929688
Testing Accuracy: 0.914062
Testing Accuracy: 0.929688
Testing Accuracy: 0.929688
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.921875
For iter  220
Accuracy  0.921875
Loss  0.27915
__________________
Testing Accuracy: 0.921875
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.929688
Testing Accuracy: 0.960938
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
For iter  230
Accuracy  0.929688
Loss  0.206641
__________________
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.953125
Testing Accuracy: 0.9375
Testing Accuracy: 0.929688
Testing Accuracy: 0.90625
For iter  240
Accuracy  0.945312
Loss  0.177758
__________________
Testing Accuracy: 0.929688
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
For iter  250
Accuracy  0.929688
Loss  0.209504
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
For iter  260
Accuracy  0.945312
Loss  0.192074
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
For iter  270
Accuracy  0.953125
Loss  0.15836
__________________
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
For iter  280
Accuracy  0.953125
Loss  0.125364
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
For iter  290
Accuracy  0.921875
Loss  0.220573
__________________
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  300
Accuracy  0.929688
Loss  0.225352
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  310
Accuracy  0.960938
Loss  0.133694
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
For iter  320
Accuracy  0.976562
Loss  0.205548
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
For iter  330
Accuracy  0.921875
Loss  0.207905
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  340
Accuracy  0.960938
Loss  0.164377
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
For iter  350
Accuracy  0.9375
Loss  0.169862
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.953125
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
For iter  360
Accuracy  0.9375
Loss  0.226911
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
For iter  370
Accuracy  0.96875
Loss  0.173299
__________________
Testing Accuracy: 0.945312
Testing Accuracy: 0.945312
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  380
Accuracy  0.945312
Loss  0.166889
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
For iter  390
Accuracy  0.953125
Loss  0.239948
__________________
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.96875
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
For iter  400
Accuracy  0.953125
Loss  0.0787421
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
For iter  410
Accuracy  0.96875
Loss  0.0669084
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
For iter  420
Accuracy  0.960938
Loss  0.116731
__________________
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 0.984375
For iter  430
Accuracy  0.976562
Loss  0.100515
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 0.992188
Testing Accuracy: 1.0
For iter  440
Accuracy  0.929688
Loss  0.191241
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
For iter  450
Accuracy  0.953125
Loss  0.137285
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 1.0
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
For iter  460
Accuracy  0.976562
Loss  0.0721269
__________________
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
For iter  470
Accuracy  0.953125
Loss  0.125116
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
For iter  480
Accuracy  0.96875
Loss  0.0720188
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
For iter  490
Accuracy  0.976562
Loss  0.118606
__________________
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
For iter  500
Accuracy  0.960938
Loss  0.158814
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  510
Accuracy  0.96875
Loss  0.106757
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
For iter  520
Accuracy  0.976562
Loss  0.0856151
__________________
Testing Accuracy: 0.953125
Testing Accuracy: 0.9375
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  530
Accuracy  0.953125
Loss  0.0772318
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
For iter  540
Accuracy  0.984375
Loss  0.0558051
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  550
Accuracy  0.984375
Loss  0.0561309
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.976562
For iter  560
Accuracy  0.96875
Loss  0.108961
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.945312
Testing Accuracy: 0.9375
Testing Accuracy: 0.945312
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
For iter  570
Accuracy  0.945312
Loss  0.22261
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.992188
For iter  580
Accuracy  0.9375
Loss  0.189915
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
For iter  590
Accuracy  0.96875
Loss  0.0798329
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
For iter  600
Accuracy  0.984375
Loss  0.064096
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
For iter  610
Accuracy  0.9375
Loss  0.208509
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
For iter  620
Accuracy  0.96875
Loss  0.10871
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
For iter  630
Accuracy  0.96875
Loss  0.191045
__________________
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.953125
Testing Accuracy: 0.96875
Testing Accuracy: 0.984375
Testing Accuracy: 1.0
For iter  640
Accuracy  0.96875
Loss  0.0773583
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
For iter  650
Accuracy  0.96875
Loss  0.0884017
__________________
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
For iter  660
Accuracy  0.929688
Loss  0.145886
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  670
Accuracy  0.96875
Loss  0.115895
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  680
Accuracy  0.976562
Loss  0.114911
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
For iter  690
Accuracy  0.96875
Loss  0.0948501
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.976562
For iter  700
Accuracy  0.953125
Loss  0.159804
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
For iter  710
Accuracy  0.992188
Loss  0.0515673
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
For iter  720
Accuracy  0.976562
Loss  0.098267
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
Testing Accuracy: 0.976562
For iter  730
Accuracy  0.953125
Loss  0.138382
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  740
Accuracy  0.976562
Loss  0.114366
__________________
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
For iter  750
Accuracy  0.96875
Loss  0.0861038
__________________
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
For iter  760
Accuracy  0.945312
Loss  0.120766
__________________
Testing Accuracy: 0.976562
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 1.0
For iter  770
Accuracy  0.96875
Loss  0.072889
__________________
Testing Accuracy: 1.0
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 1.0
For iter  780
Accuracy  0.984375
Loss  0.0480433
__________________
Testing Accuracy: 1.0
Testing Accuracy: 1.0
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.992188
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.984375
Testing Accuracy: 0.96875
For iter  790
Accuracy  0.96875
Loss  0.101997
__________________
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.960938
Testing Accuracy: 0.960938
Testing Accuracy: 0.96875
Testing Accuracy: 0.976562
Testing Accuracy: 0.96875
Testing Accuracy: 0.96875
</code></pre><h2 id="注意（一）"><a href="#注意（一）" class="headerlink" title="注意（一）"></a>注意（一）</h2><p>我们的图像基本上被平坦化为一个单一的维度矢量784开始。功能next_batch(batch_size)必然返回batch_size这些784维度向量的批次，因此它们被重塑为[batch_size,time_steps,n_input]可以被占位符接受。</p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[DL by Andrew NG ，programming work - Initialization]]></title>
      <url>/2017/10/29/DL-PW-initialization/</url>
      <content type="html"><![CDATA[<p>这是NG的深度学习编程作业的内容。此篇文章主要是讲权重和偏置量的初始化方式。</p>
<ul>
<li>Zeros initialization - 初始化全部为零</li>
<li>Random initialization -随机初始化</li>
<li>He initialization - 随机初始化乘以 $\sqrt{\frac{2}{\text{之前的层的维度（矩阵大小）}}}$</li>
</ul>
<p>A well chosen initialization can:</p>
<ul>
<li>Speed up the convergence of gradient descent</li>
<li>Increase the odds of gradient descent converging to a lower training (and generalization) error </li>
</ul>
<p>To get started, run the following cell to load the packages and the planar dataset you will try to classify.</p>
<hr>
<h2 id="1-Neural-Network-model"><a href="#1-Neural-Network-model" class="headerlink" title="1 - Neural Network model"></a>1 - Neural Network model</h2><p>You will use a 3-layer neural network (already implemented for you). Here are the initialization methods you will experiment with:  </p>
<ul>
<li><em>Zeros initialization</em> –  setting <code>initialization = &quot;zeros&quot;</code> in the input argument.</li>
<li><em>Random initialization</em> – setting <code>initialization = &quot;random&quot;</code> in the input argument. This initializes the weights to large random values.  </li>
<li><em>He initialization</em> – setting <code>initialization = &quot;he&quot;</code> in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. </li>
</ul>
<p><strong>Instructions</strong>: Please quickly read over the code below, and run it. In the next part you will implement the three initialization methods that this <code>model()</code> calls.</p>
<hr>
<h2 id="2-Zero-initialization"><a href="#2-Zero-initialization" class="headerlink" title="2 - Zero initialization"></a>2 - Zero initialization</h2><p>There are two types of parameters to initialize in a neural network:</p>
<ul>
<li>the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, …, W^{[L-1]}, W^{[L]})$</li>
<li>the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, …, b^{[L-1]}, b^{[L]})$</li>
</ul>
<p><strong>Exercise</strong>: Implement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry”, but lets try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros([layers_dims[l],layers_dims[l<span class="number">-1</span>]])</div><div class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros([layers_dims[l],<span class="number">1</span>])</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div></pre></td></tr></table></figure>
<hr>
<font color="blue"><br><strong>What you should remember</strong>:<br>- The weights $W^{[l]}$ should be initialized randomly to break symmetry.<br>- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly.<br></font>

<hr>
<h2 id="3-Random-initialization"><a href="#3-Random-initialization" class="headerlink" title="3 - Random initialization"></a>3 - Random initialization</h2><p>To break symmetry, lets intialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you will see what happens if the weights are intialized randomly, but to very large values. </p>
<p><strong>Exercise</strong>: Implement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use <code>np.random.randn(..,..) * 10</code> for weights and <code>np.zeros((.., ..))</code> for biases. We are using a fixed <code>np.random.seed(..)</code> to make sure your “random” weights  match ours, so don’t worry if running several times your code gives you always the same initial values for the parameters. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>]) * <span class="number">10</span></div><div class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros([layers_dims[l],<span class="number">1</span>])</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div></pre></td></tr></table></figure>
<hr>
<h2 id="4-He-initialization"><a href="#4-He-initialization" class="headerlink" title="4 - He initialization"></a>4 - He initialization</h2><p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of <code>sqrt(1./layers_dims[l-1])</code> where He initialization would use <code>sqrt(2./layers_dims[l-1])</code>.)</p>
<p><strong>Exercise</strong>: Implement the following function to initialize your parameters with He initialization.</p>
<p><strong>Hint</strong>: This function is similar to the previous <code>initialize_parameters_random(...)</code>. The only difference is that instead of multiplying <code>np.random.randn(..,..)</code> by 10, you will multiply it by $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class="number">-1</span>]) * np.sqrt(<span class="number">2.</span>/layers_dims[l<span class="number">-1</span>])</div><div class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros([layers_dims[l],<span class="number">1</span>])</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div></pre></td></tr></table></figure>
<hr>
<h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5 - Conclusions"></a>5 - Conclusions</h2><p>You have seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:</p>
<table><br>    <tr><br>        <td><br>        <strong>Model</strong><br>        </td><br>        <td><br>        <strong>Train accuracy</strong><br>        </td><br>        <td><br>        <strong>Problem/Comment</strong><br>        </td><br><br>    </tr><br>        <td><br>        3-layer NN with zeros initialization<br>        </td><br>        <td><br>        50%<br>        </td><br>        <td><br>        fails to break symmetry<br>        </td><br>    <tr><br>        <td><br>        3-layer NN with large random initialization<br>        </td><br>        <td><br>        83%<br>        </td><br>        <td><br>        too large weights<br>        </td><br>    </tr><br>    <tr><br>        <td><br>        3-layer NN with He initialization<br>        </td><br>        <td><br>        99%<br>        </td><br>        <td><br>        recommended method<br>        </td><br>    </tr><br></table> 

<font color="red"><br><strong>What you should remember from this notebook</strong>:<br>- Different initializations lead to different results<br>- Random initialization is used to break symmetry and make sure different hidden units can learn different things<br>- Don’t intialize to values that are too large<br>- He initialization works well for networks with ReLU activations.<br></font>



]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[deeplearning-ai]]></title>
      <url>/2017/10/27/deeplearning-ai/</url>
      <content type="html"><![CDATA[<p>deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程<strong>笔记</strong></p>
<h2 id="神经网络和深度学习"><a href="#神经网络和深度学习" class="headerlink" title="神经网络和深度学习"></a>神经网络和深度学习</h2><h3 id="第一周-深度学习概论"><a href="#第一周-深度学习概论" class="headerlink" title="第一周 深度学习概论"></a>第一周 深度学习概论</h3><ul>
<li><p>1.1 欢迎来到深度学习工程师微专业</p>
</li>
<li><p>1.2 什么是神经网络？</p>
</li>
<li><p>1.3 用神经网络进行监督学习</p>
</li>
<li><p>1.4 为什么神经网络会兴起</p>
</li>
<li><p>1.5 关于这门课</p>
</li>
<li><p>1.6 课程资源</p>
</li>
</ul>
<h3 id="第二周-神经网络基础"><a href="#第二周-神经网络基础" class="headerlink" title="第二周 神经网络基础"></a>第二周 神经网络基础</h3><ul>
<li><p>2.1 二元分类</p>
</li>
<li><p>2.2 logistic 回归</p>
</li>
<li><p>2.3 logistic 回归损失函数</p>
</li>
<li><p>2.4 梯度下降法</p>
</li>
<li><p>2.5 导数</p>
</li>
<li><p>2.6 更多导数的例子</p>
</li>
<li><p>2.7 计算图</p>
</li>
<li><p>2.8 计算图的导数运算</p>
</li>
<li><p>2.9 logistic 回归中的梯度下降法</p>
</li>
<li><p>2.10 m个样本的梯度下降</p>
</li>
<li><p>2.11 向量化</p>
</li>
<li><p>2.12 向量化的更多例子</p>
</li>
<li><p>2.13 向量化的logistic 回归</p>
</li>
<li><p>2.14 向量化logistic 回归的梯度输出</p>
</li>
<li><p>2.15 Python中的广播</p>
</li>
<li><p>2.16 关于Python/numpy向量的说明</p>
</li>
<li><p>2.17 Jupiter/IPython笔记本的快速指南</p>
</li>
<li><p>2.18 （选修）logistic损失函数的解释</p>
</li>
</ul>
<h3 id="第三周-浅层神经网络"><a href="#第三周-浅层神经网络" class="headerlink" title="第三周 浅层神经网络"></a>第三周 浅层神经网络</h3><ul>
<li><p>3.1 神经网络概览</p>
</li>
<li><p>3.2 神经网络表示</p>
</li>
<li><p>3.3 计算神经网络的输出</p>
</li>
<li><p>3.4 多样本向量化</p>
</li>
<li><p>3.5 向量化实现的解释</p>
</li>
<li><p>3.6 激活函数</p>
</li>
<li><p>3.7 为什么需要非线性激活函数</p>
</li>
<li><p>3.8 激活函数的导数</p>
</li>
<li><p>3.9 神经网络的梯度下降法</p>
</li>
<li><p>3.10 （选修）直观理解反向传播</p>
</li>
<li><p>3.11 随机初始化</p>
</li>
</ul>
<h3 id="第四周-深层神经网络"><a href="#第四周-深层神经网络" class="headerlink" title="第四周 深层神经网络"></a>第四周 深层神经网络</h3><ul>
<li><p>4.1 深层神经网络</p>
</li>
<li><p>4.2 深层网络中的前向传播</p>
</li>
<li><p>4.3 核对矩阵的维数</p>
</li>
<li><p>4.4 为什么使用深层表示</p>
</li>
<li><p>4.5 搭建深层神经网络块</p>
</li>
<li><p>4.6 前向和反向传播</p>
</li>
<li><p>4.7 参数 VS 超参数</p>
</li>
<li><p>4.8 这和大脑有什么关系</p>
</li>
</ul>
<h3 id="大师访谈"><a href="#大师访谈" class="headerlink" title="大师访谈"></a>大师访谈</h3><ul>
<li><p>Geoffrey Hinton</p>
</li>
<li><p>Pieter Abbeel</p>
</li>
<li><p>Ian Goodfellow</p>
</li>
</ul>
<h2 id="改善深层神经网络：超参数调试、正则化以及优化"><a href="#改善深层神经网络：超参数调试、正则化以及优化" class="headerlink" title="改善深层神经网络：超参数调试、正则化以及优化"></a>改善深层神经网络：超参数调试、正则化以及优化</h2><h3 id="第一周-深度学习的实用层面"><a href="#第一周-深度学习的实用层面" class="headerlink" title="第一周 深度学习的实用层面"></a>第一周 深度学习的实用层面</h3><ul>
<li><p>1.1 训练/开发/测试集</p>
</li>
<li><p>1.2 偏差/方差</p>
</li>
<li><p>1.3 机器学习基础</p>
</li>
<li><p>1.4 正则化</p>
</li>
<li><p>1.5 为什么正则化可以减少过拟合</p>
</li>
<li><p>1.6 Dropout 正则化</p>
<ul>
<li><p>定义</p>
<ul>
<li>从基础网络中除去随机除去非输出单元后形成的子网络</li>
</ul>
</li>
<li><p>理解</p>
<ul>
<li><p>通过随机行为训练网络并平均多个随机决定进行预测，实现了一种参数共享形式的bagging形式</p>
</li>
<li><p>类似于遗传算法</p>
</li>
</ul>
</li>
<li><p>成功原因</p>
<ul>
<li>施加到隐藏层的掩码噪声，使得隐藏层的特征受到破坏，增加了模型的泛化能力</li>
</ul>
</li>
</ul>
</li>
<li><p>1.7 理解Dropout</p>
</li>
<li><p>1.8 其他正则化方法</p>
</li>
<li><p>1.9 正则化输入</p>
</li>
<li><p>1.10 梯度消失和梯度爆炸</p>
<ul>
<li><p>原因</p>
<ul>
<li>传播时参数W多次相乘导致产生指数级的梯度。</li>
</ul>
</li>
</ul>
</li>
<li><p>1.11 神经网络的权重初始化</p>
</li>
<li><p>1.12 梯度的数值逼近</p>
</li>
<li><p>1.13 梯度检验</p>
</li>
<li><p>1.14 关于梯度检验实现的注记</p>
</li>
</ul>
<h3 id="第二周-优化算法"><a href="#第二周-优化算法" class="headerlink" title="第二周 优化算法"></a>第二周 优化算法</h3><ul>
<li><p>2.1 Mini-batch 梯度下降法</p>
</li>
<li><p>2.2 理解mini-batch梯度下降法</p>
</li>
<li><p>2.3 指数加权平均</p>
</li>
<li><p>2.4 理解指数加权平均</p>
</li>
<li><p>2.5 指数加权平均的偏差修正</p>
</li>
<li><p>2.6 动量梯度下降法</p>
</li>
<li><p>2.7 RMSprop</p>
</li>
<li><p>2.8 Adam优化算法</p>
</li>
<li><p>2.9 学习率衰减</p>
</li>
<li><p>2.10 局部优化的问题</p>
</li>
</ul>
<h3 id="第三周-超参数、Batch正则化和程序框架"><a href="#第三周-超参数、Batch正则化和程序框架" class="headerlink" title="第三周 超参数、Batch正则化和程序框架"></a>第三周 超参数、Batch正则化和程序框架</h3><ul>
<li><p>3.1 调试处理</p>
</li>
<li><p>3.2 为超参数选择合适的范围</p>
</li>
<li><p>3.3 超参数训练的实践：pandas VS Caviar</p>
</li>
<li><p>3.4 正则化网络的激活函数</p>
</li>
<li><p>3.5 将Batch Norm 拟合进神经网络</p>
</li>
<li><p>3.6 Batch Norm为什么奏效</p>
</li>
<li><p>3.7 测试时的 Batch norm</p>
</li>
<li><p>3.8 Softmax 回归</p>
</li>
<li><p>3.9 训练一个softmax分类器</p>
</li>
<li><p>3.10 深度学习框架</p>
</li>
<li><p>3.11 TensorFlow</p>
</li>
</ul>
<h3 id="大师访谈-1"><a href="#大师访谈-1" class="headerlink" title="大师访谈"></a>大师访谈</h3><ul>
<li><p>Yoshua Bengio<br>加拿大蒙特利尔大学教授，蒙特利尔大学机器学习研究所（MILA）的负责人，神经网络三巨头之一，人工智能孵化器 Element AI 联合创始人之一，2017年成为微软人工智能研究顾问。</p>
</li>
<li><p>林元庆<br>现任百度深度学习实验室（IDL）主任，曾任 NEC 美国实验室媒体分析部门主管。在他的带领下 NEC 研究团队在深度学习、计算机视觉和无人驾驶等领域取得世界领先水平。</p>
</li>
</ul>
<h2 id="结构化机器学习项目"><a href="#结构化机器学习项目" class="headerlink" title="结构化机器学习项目"></a>结构化机器学习项目</h2><h3 id="第一周-机器学习（ML）策略（1）"><a href="#第一周-机器学习（ML）策略（1）" class="headerlink" title="第一周 机器学习（ML）策略（1）"></a>第一周 机器学习（ML）策略（1）</h3><ul>
<li><p>1.1 什么是ML策略</p>
</li>
<li><p>1.2 正交化</p>
</li>
<li><p>1.3 单一数字评估指标</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac anaconda|Miniconda OpenCV环境配置]]></title>
      <url>/2017/10/27/Mac_anaconda_OpenCV%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>Mac本来就是小众系统，专门拿Mac来码代码就更少了。这就造成了网上对一些环境的配置的教程很少。然后开源的库配置起来简直就是坑啊，巨坑。不说了，说多了都是泪。只是希望本篇文章能给Mac“玩家”带来一些方便。</p>
<h2 id="总的环境"><a href="#总的环境" class="headerlink" title="总的环境"></a>总的环境</h2><ul>
<li>系统版本：macOS Sierra 10.12.6</li>
<li>Python版本 ：Python 2.7.13</li>
<li>conda版本：conda 4.3.22</li>
</ul>
<hr>
<ul>
<li>选择的OpenCV版本：<strong>opencv 3.1.0</strong>（这很重要，我就是因为选择了opencv2.4，还有opencv3.2的版本始终依赖缺失）</li>
</ul>
<h2 id="1-选择一个国内的镜像"><a href="#1-选择一个国内的镜像" class="headerlink" title="1.选择一个国内的镜像"></a>1.选择一个国内的镜像</h2><ul>
<li><p>原因：河蟹神兽实在太厉害了，选择conda的默认镜像网站下载，不仅仅是慢的问题，而且经常出现断流，直接报错中断下载了。<br>假如你出现了conda、 http、 error等单词在终端的报错内容里出现，请换一个镜像网站。</p>
</li>
<li><p>推荐：<a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="external">清华大学开源软件镜像站</a><br>清华大学的镜像站不止有anaconda的库，还有常见的Ubuntu之类的，实在太好用了。安利一波~~~~<br>不知道为什么，清华维护的Conda 三方源反而下载opencv的速度更快。<br>推荐使用一下代码添加镜像网站：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</div><div class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</div></pre></td></tr></table></figure>
<h2 id="2-测试一下镜像速度"><a href="#2-测试一下镜像速度" class="headerlink" title="2.测试一下镜像速度"></a>2.测试一下镜像速度</h2><ul>
<li>运行<code>conda install numpy</code><br>以下是测试结果：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">zhangdeMacBook-Air:~ zhang$ conda install numpy</div><div class="line">Fetching package metadata .......</div><div class="line">Solving package specifications: .</div><div class="line"></div><div class="line">Package plan for installation in environment /Users/zhang/miniconda2:</div><div class="line"></div><div class="line">The following NEW packages will be INSTALLED:</div><div class="line"></div><div class="line">    mkl:       2017.0.3-0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</div><div class="line">    numpy:     1.13.1-py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</div><div class="line"></div><div class="line">The following packages will be UPDATED:</div><div class="line"></div><div class="line">    conda:     4.3.21-py27_0 defaults                                                --&gt; 4.3.22-py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</div><div class="line"></div><div class="line">The following packages will be SUPERSEDED by a higher-priority channel:</div><div class="line"></div><div class="line">    conda-env: 2.6.0-0       defaults                                                --&gt; 2.6.0-0       https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</div><div class="line"></div><div class="line">Proceed ([y]/n)? y</div><div class="line"></div><div class="line">conda-env-2.6. 100% |################################| Time: 0:00:00  62.22 kB/s</div><div class="line">mkl-2017.0.3-0 100% |################################| Time: 0:01:10   1.65 MB/s</div><div class="line">numpy-1.13.1-p 100% |################################| Time: 0:00:01   2.73 MB/s</div><div class="line">conda-4.3.22-p 100% |################################| Time: 0:00:00   2.71 MB/s</div><div class="line">zhangdeMacBook-Air:~ zhang$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</div><div class="line">zhangdeMacBook-Air:~ zhang$ conda install pandas</div><div class="line">Fetching package metadata .......</div><div class="line">Solving package specifications: .</div><div class="line"></div><div class="line">Package plan for installation in environment /Users/zhang/miniconda2:</div><div class="line"></div><div class="line">The following NEW packages will be INSTALLED:</div><div class="line"></div><div class="line">    pandas:          0.20.3-py27_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line">    python-dateutil: 2.6.1-py27_0  https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line">    pytz:            2017.2-py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line"></div><div class="line">The following packages will be SUPERSEDED by a higher-priority channel:</div><div class="line"></div><div class="line">    conda:           4.3.22-py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free         --&gt; 4.3.22-py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line">    conda-env:       2.6.0-0       https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free         --&gt; 2.6.0-0       https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line"></div><div class="line">Proceed ([y]/n)? y</div><div class="line"></div><div class="line">conda-env-2.6. 100% |################################| Time: 0:00:00 110.49 kB/s</div><div class="line">pytz-2017.2-py 100% |################################| Time: 0:00:00   1.83 MB/s</div><div class="line">python-dateuti 100% |################################| Time: 0:00:00   2.12 MB/s</div><div class="line">pandas-0.20.3- 100% |################################| Time: 0:00:04   2.41 MB/s</div><div class="line">conda-4.3.22-p 100% |################################| Time: 0:00:00   2.26 MB/s</div></pre></td></tr></table></figure>
<h2 id="3-conda安装opencv"><a href="#3-conda安装opencv" class="headerlink" title="3. conda安装opencv"></a>3. conda安装opencv</h2><ol>
<li>尝试使用<code>conda install opencv</code> 安装<br>默认安装的是opencv 3.2.0<br>结果：安装成功了？？？？</li>
<li>进入Python界面看看<ul>
<li>使用ipython， 进入增强版Python交互界面</li>
<li>使用 <code>import cv2</code>导入 opencv的模块</li>
<li>显然失败了<br>我也试图用其他工具例如homebrew安装，无一例外都失败了。也许3.2.0有什么bug吧。</li>
</ul>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">zhangdeMacBook-Air:~ zhang$ ipython</div><div class="line">Python 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) </div><div class="line">Type "copyright", "credits" or "license" for more information.</div><div class="line"></div><div class="line">IPython 5.4.1 -- An enhanced Interactive Python.</div><div class="line">?         -&gt; Introduction and overview of IPython's features.</div><div class="line"><span class="meta">%</span>quickref -&gt; Quick reference.</div><div class="line">help      -&gt; Python's own help system.</div><div class="line">object?   -&gt; Details about 'object', use 'object??' for extra details.</div><div class="line"></div><div class="line">In [1]: import cv2</div><div class="line">---------------------------------------------------------------------------</div><div class="line">ImportError                               Traceback (most recent call last)</div><div class="line">&lt;ipython-input-1-72fbbcfe2587&gt; in &lt;module&gt;()</div><div class="line"><span class="meta">----&gt;</span> 1 import cv2</div><div class="line"></div><div class="line">ImportError: dlopen(/Users/zhang/miniconda2/lib/python2.7/site-packages/cv2.so, 2): Library not loaded: @rpath/libopenblasp-r0.2.19.dylib</div><div class="line">  Referenced from: /Users/zhang/miniconda2/lib/libopencv_hdf.3.2.0.dylib</div><div class="line">  Reason: image not found</div></pre></td></tr></table></figure>
<p>3.使用 <code>conda install opencv=3.1.0</code> 安装<br>看看结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">zhangdeMacBook-Air:~ zhang$ conda install opencv=3.1.0</div><div class="line">Fetching package metadata .......</div><div class="line">Solving package specifications: .</div><div class="line"></div><div class="line">Package plan for installation in environment /Users/zhang/miniconda2:</div><div class="line"></div><div class="line">The following packages will be DOWNGRADED:</div><div class="line"></div><div class="line">    opencv: 3.2.0-np112py27_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge --&gt; 3.1.0-np112py27_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</div><div class="line"></div><div class="line">Proceed ([y]/n)? y</div><div class="line"></div><div class="line">opencv-3.1.0-n 100% |################################| Time: 0:00:15   2.51 MB/s</div></pre></td></tr></table></figure>
<ul>
<li>显然又一次成功安装了，再来看看Python能否导入</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">zhangdeMacBook-Air:~ zhang$ ipython</div><div class="line">Python 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) </div><div class="line">Type "copyright", "credits" or "license" for more information.</div><div class="line"></div><div class="line">IPython 5.4.1 -- An enhanced Interactive Python.</div><div class="line">?         -&gt; Introduction and overview of IPython's features.</div><div class="line"><span class="meta">%</span>quickref -&gt; Quick reference.</div><div class="line">help      -&gt; Python's own help system.</div><div class="line">object?   -&gt; Details about 'object', use 'object??' for extra details.</div><div class="line"></div><div class="line">In [1]: import cv2</div><div class="line"></div><div class="line">In [2]: exit</div></pre></td></tr></table></figure>
<ul>
<li>命令<code>import cv2</code> 导入opencv成功</li>
</ul>
<h2 id="4-总结一下"><a href="#4-总结一下" class="headerlink" title="4.总结一下"></a>4.总结一下</h2><p>命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>为了加快下载速度使用清华的镜像</div><div class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</div><div class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</div><div class="line"><span class="meta">#</span>最新版3.2.0有毒啊，只能3.1.0</div><div class="line">conda install opencv=3.1.0</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> 编程问题 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[损失函数初解]]></title>
      <url>/2017/08/14/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%88%9D%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>经常在机器学习或者深度学习的概念中提到损失函数（loss function），其实它最初来自统计学领域。让我们初步探索一下损失函数的概念吧！</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="统计学领域："><a href="#统计学领域：" class="headerlink" title="统计学领域："></a>统计学领域：</h3><p>损失函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数。</p>
<h3 id="通俗理解："><a href="#通俗理解：" class="headerlink" title="通俗理解："></a>通俗理解：</h3><p>损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。</p>
<h3 id="机器学习："><a href="#机器学习：" class="headerlink" title="机器学习："></a>机器学习：</h3><p>在有监督学习范畴，我们会给定电脑一组正确的输入和输出（理解为y 和 x ），在人脸识别领域这组值是无数的人脸图片和人脸的身份标签。</p>
<p>然后，我们给电脑一个公式  ，例如：<code>f(x) = Wx + b</code> 。然后电脑不停地计算，求出符合符合公式的 W 。</p>
<p>那么问题来了，电脑如何求 W 的值。W 是大了，还是小了，依据什么判断？</p>
<p>这个问题的解决方法很简单，由于已知 y 这个正确的值，我们可以用 <code>(f(x) - y)^2</code> （只是方法之一）来观察 W 的值应该怎么调整。</p>
<p>因此，产生了 <code>loss(f(x), y)</code>这个称为损失函数的东西。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="机器学习：-1"><a href="#机器学习：-1" class="headerlink" title="机器学习："></a>机器学习：</h3><p>在分类和回归问题中常用于优化真实值和预测值得差距</p>
<h3 id="深度学习："><a href="#深度学习：" class="headerlink" title="深度学习："></a>深度学习：</h3><p>深度学习采用多层感知机来处理一些图像语音等特征，但最终往往会回归到分类或者回归问题。因此在神经网络的输出层之后一般会有损失函数参与优化。</p>
<h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><p>常见的损失函数有</p>
<h2 id="1-多重支持向量机"><a href="#1-多重支持向量机" class="headerlink" title="1. 多重支持向量机"></a>1. 多重支持向量机</h2><h2 id="2-softmax-函数"><a href="#2-softmax-函数" class="headerlink" title="2. softmax 函数"></a>2. softmax 函数</h2>]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络入门]]></title>
      <url>/2017/08/14/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<p>本篇文章主要介绍梯度下降、反向传播、卷积和池化几个概念。</p>
<h2 id="1-梯度下降（Gradient-Descent）"><a href="#1-梯度下降（Gradient-Descent）" class="headerlink" title="1.梯度下降（Gradient Descent）"></a>1.梯度下降（Gradient Descent）</h2><ul>
<li>梯度：函数在某一点的梯度是一个自变量空间内的向量。自变量顺着梯度方向变化时函数值上升得最快。梯度的模（长度）是函数值上升的速率。梯度朝某方向投影的长度是自变量顺着该方向变化时函数值的变化率.</li>
</ul>
<ul>
<li>梯度下降算法：在现实世界中，梯度可以更为直观的理解为坡度。想象在一座山坡上，你想最快地下山。山坡越陡峭，迈相同的步子下山就越快。当然，在机器学习的相关模型中，我们无法选择梯度（山坡的坡度），起始点是可以选择的（在哪个位置下山），学习效率（learning rate，就是下山的步子大小）也是可以选择的。<br>根据上面的例子，我们可以理解为我们在山坡的某一点要到山下某一个平地上（坡度为零）。</li>
</ul>
<h3 id="为什么要梯度下降"><a href="#为什么要梯度下降" class="headerlink" title="为什么要梯度下降"></a>为什么要梯度下降</h3><p> 梯度下降算法是为了解决最小化损失函数而提出的。损失函数可以参考？。简单地说，我们希望模型预测值和真实值的误差越小越好，所以我们用损失函数来衡量这些误差。</p>
<p> 损失函数既然是一个函数，我们在坐标系中可以将它描绘出来（只要它没有超过三维，超过好像也是有办法描绘的，不够不太好理解）。我们举最简单例子（纵坐标表示损失函数的大小，横坐标表示可以调整的参数）：</p>
<p><img src="http://i2.bvimg.com/605305/9cb4053c54f871b6t.jpg" alt="Markdown"></p>
<p>如果我们在这个函数的山坡上，想到达海拔最低的地方。显然，我们会到达横坐标为3附近的点。因为这个地方是坡底，坡度为0，也就是梯度为零。</p>
<p>推广到三维坐标系下，我们可以看看：</p>
<p><img src="http://i2.bvimg.com/605305/4fd05ea715845268t.jpg" alt="Markdown"></p>
<p>在不同的出发点出发，我们根据坡度最陡峭的方向（梯度）往山下走，如图所示，我们可能会走到不同的坡底。这是因为我们往往无法知道全局的形状，指引我们下山的是坡度（梯度）。因此，我们可能会遇到局部最小值，也就是说，我们跑到了坡底，我们无法知道有没有另一个坡底，比这个坡底海拔还要低。这也是梯度下降算法的一个弊端：无法确定是否是全局最小，而不是局部最小。</p>
<h2 id="2-反向传播（Back-propagation）"><a href="#2-反向传播（Back-propagation）" class="headerlink" title="2.反向传播（Back propagation）"></a>2.反向传播（Back propagation）</h2><ul>
<li>概念：反向传播算法是应用于神经网络，为了解决深层参数训练问题而提出的。<em>它的本质是通过链式法则求得神经网络中所有节点的梯度。</em></li>
</ul>
<h2 id="3-卷积与池化"><a href="#3-卷积与池化" class="headerlink" title="3.卷积与池化"></a>3.卷积与池化</h2><p>卷积层：对图像进行卷积操作。</p>
<p>池化层：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，一般在ixi的区域内取平均或最大值，作为新的特征图的元素。同时，池化层还有防止过度拟合的作用。</p>
<p><img src="http://i2.bvimg.com/605305/f44fab62544dee47.png" alt="Markdown"></p>
<p>如上如图所示，这是max pool的方式进行池化，即取最大值的方式。可以看到，左侧4x4矩阵的右上角2x2的部分，其最大值是6，对应右侧矩阵第一个元素。</p>
<h2 id="4-卷积神经网络的一般流程"><a href="#4-卷积神经网络的一般流程" class="headerlink" title="4.卷积神经网络的一般流程"></a>4.卷积神经网络的一般流程</h2><p><img src="http://i2.bvimg.com/605305/7e3aecf357885d2c.png" alt="Markdown"></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谷歌图标加载解决方案]]></title>
      <url>/2017/08/14/%E8%B0%B7%E6%AD%8C%E5%9B%BE%E6%A0%87%E5%8A%A0%E8%BD%BD%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      <content type="html"><![CDATA[<p>默认的Material原质主题由于采用了谷歌的图标，被墙了。换一个字体图标库就可以了。</p>
<h2 id="问题由来"><a href="#问题由来" class="headerlink" title="问题由来"></a>问题由来</h2><p><a href="https://github.com/viosey/hexo-theme-material" target="_blank" rel="external">hexo-theme-material</a>主题默认采用了谷歌的字体图标。<br>我大中华的河蟹神兽专治谷歌的各种糖衣炮弹。显然，被和谐了。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>在 <a href="https://material.viosey.com/" target="_blank" rel="external">Material原质</a> 的官网中，子界面的 <a href="https://material.viosey.com/intro/" target="_blank" rel="external">配置介绍</a> 里，有一段关于字体的介绍：</p>
<blockquote>
<p>use: 用于设置站点字体的引用方式。Material 主题内置了以下三种字体库支持。除此以外，你也可以手动设定你喜欢的谷歌字体反代服务。</p>
</blockquote>
<pre><code>•    google: 使用 Google 字体库加载 Roboto 字体和 Material Icon。当使用了 Isolation UX 时，主题会从本地或 MaterialCDN 加载 Font-Awesome
•    ustc: 使用中科大反代的 Google 字体库加载 Roboto 字体和 Material Icon。当使用了 Isolation UX 时，主题会从本地或 MaterialCDN 加载 Font-Awesome
•    baomitu: 使用 360 前端团队 奇舞团 维护的字体库加载 Roboto 字体和 Material Icon。当使用了 Isolation UX 时，主题会从 奇舞团 维护的公共 CDN 库加载 Font-Awesome
•    custom, 使用你喜欢的谷歌字体反代服务加载 Roboto 字体和 Material Icon。使用该选项需要在 custom_font_host 中填入字体库的 URL。当使用了 Isolation UX 时，主题会从本地或 MaterialCDN 加载 Font-Awesome
</code></pre><p>默认的字体采用了<code>Google</code>，我们只要改成 <code>ustc</code>就可以了。如下图：</p>
<p><img src="http://i2.fuimg.com/605305/756a3e4553bd0342.png" alt="Markdown"></p>
]]></content>
      
        <categories>
            
            <category> 建站问题 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 建站问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[新生]]></title>
      <url>/2017/08/14/%E6%96%B0%E7%94%9F/</url>
      <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=480178467&auto=1&height=66"></iframe>

<p>折腾了好久，终于搞定了这个个人博客。首先得发表一下感谢：感谢党，感谢人民，还有感谢<a href="https://github.com/viosey/hexo-theme-material" target="_blank" rel="external">Viosey</a>开源的主题。</p>
<h2 id="Bugs"><a href="#Bugs" class="headerlink" title="Bugs"></a>Bugs</h2><p>网页上的一些谷歌material图标被墙了，加载出来的是英文。Viosey大神的团队成员据说会尽快解决。</p>
<p>可惜虽然身为程序猿，但不是学前端的，很无奈啊。Google试试吧！</p>
<h2 id="调整"><a href="#调整" class="headerlink" title="调整"></a>调整</h2><p>因为用了第三方的主题包，很多地方需要调整，慢慢来吧。</p>
<h2 id="起航"><a href="#起航" class="headerlink" title="起航"></a>起航</h2><p>虽然小船破了些，但目标可是星辰大海！</p>
]]></content>
      
        
    </entry>
    
  
  
</search>
